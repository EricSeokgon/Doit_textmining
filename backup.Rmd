<!-- ## 5.2 동시 출현 네트워크 - Co-occurrence network -->

<!-- 단어들이 서로 얼마나 가까운지 네트워크 형태로 표현한 것을 **동시 출현 네트워크(Co-occurrence network)** 라고 합니다. 단어 동시 출현 빈도를 이용해 동시 출현 네트워크를 만들면 단어들이 어떤 맥락에서 사용됐는지 이해할 수 있습니다. -->

<!-- ### 5.2.1 네트워크 그래프 데이터 만들기 - `as_tbl_graph()` -->

<!-- 동시 출현 네트워크를 만들려면 동시 출현 빈도 데이터를 '네트워크 그래프 데이터'로 변환해야 합니다. `tidygraph` 패키지의 `as_tbl_graph()`를 이용하면 네트워크 그래프 데이터를 만들 수 있습니다. -->

<!-- 네트워크가 너무 복잡하게 구성되지 않도록 단어 동시 출현 빈도를 담고 있는 `pair`에서 25회 이상 사용된 단어만 추출해 네트워크 그래프 데이터를 생성하겠습니다. 생성된 `graph_comment`를 출력하면 단어를 나타내는 노드(Node, 꼭짓점) 30개와 단어를 연결하는 엣지(Edge, 선) 108개로 구성된 것을 확인할 수 있습니다. 그래프를 만들 때 이 값들을 활용합니다. -->

<!-- ```{r eval=F} -->
<!-- install.packages("tidygraph") -->
<!-- library(tidygraph) -->

<!-- graph_comment <- pair %>% -->
<!--   filter(n >= 25) %>% -->
<!--   as_tbl_graph() -->

<!-- graph_comment -->
<!-- ``` -->


<!-- ```{r echo=F} -->
<!-- # install.packages("tidygraph") -->
<!-- library(tidygraph) -->

<!-- graph_comment <- pair %>% -->
<!--   filter(n >= 25) %>% -->
<!--   as_tbl_graph() -->

<!-- graph_comment -->
<!-- ``` -->

<!-- > [참고] 분석 결과물을 확인하기 전에는 빈도가 몇 이상인 단어를 추출하는 게 적당한지 알 수 없습니다. 빈도를 조절하며 그래프를 여러 번 만들어 본 다음 적당한 값을 찾아야 합니다. -->




<!-- ### 5.2.2 네트워크 그래프 만들기 - `ggraph()` -->

<!-- `graph_comment`를 이용해 네트워크 그래프를 만들겠습니다. `ggraph` 패키지의 `ggraph()`를 이용하면 네트워크 그래프를 쉽게 만들 수 있습니다. 네트워크 그래프는 엣지와 노드로 구성됩니다. (1) `ggraph()`에 앞에서 생성한 `graph_comment`를 입력하고 `geom_edge_link()`를 추가하면 단어가 엣지로 연결됩니다. (2) 여기에 `geom_node_point()`를 추가하면 각 단어가 노드로 구성됩니다. (3) 마지막으로 `geom_node_text()`를 추가하고 `aes(label = name)`을 입력하면 `graph_comment`의 `name`에 있는 단어가 노드에 표시됩니다.  -->

<!-- ```{r eval=F} -->
<!-- install.packages("ggraph") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(ggraph) -->
<!-- ggraph(graph_comment) + -->
<!--   geom_edge_link() +                 # 엣지 -->
<!--   geom_node_point() +                # 노드 -->
<!--   geom_node_text(aes(label = name))  # 텍스트 -->
<!-- ``` -->

<!-- > [알아두면 좋아요] 그래프를 큰 화면에 출력하는 방법 -->

<!-- > 네트워크 그래프는 큰 이미지로 봐야만 전체 구성을 확인할 수 있습니다. RStudio의 Plots 창에서 Zoom 아이콘을 클릭 해 큰 화면에서 살펴보세요. 또는 이미지 출력 창을 별도로 띄우면 큰 화면에서 그래프를 살펴볼 수 있습니다. 윈도우에서는 `windows()`, macOS에서는 `x11()`을 실행하고 창을 크게 만든 다음 그래프를 출력해보세요. -->


<!-- #### 그래프 다듬기 -->

<!-- 네트워크 그래프를 보기 좋게 수정하겠습니다. 우선 노드의 한글을 표현하는데 사용할 폰트를 설정하겠습니다. -->

<!-- ```{r} -->
<!-- library(showtext) -->
<!-- font_add_google(name = "Nanum Gothic", family = "nanumgothic") -->
<!-- showtext_auto() -->
<!-- ``` -->

<!-- 이제 함수의 파라미터를 이용해 엣지와 노드의 색깔, 크기, 텍스트 표시 위치 등을 수정하겠습니다. `ggraph()`의 `layout`은 네트워크 형태를 지정하는 기능을 합니다. `layout`을 지정하면 난수를 이용해 매번 다른 모양의 그래프를 만듭니다. `set.seed()`로 난수를 고정해 항상 같은 모양의 그래프를 만들도록 설정하겠습니다. -->

<!-- ```{r} -->
<!-- set.seed(1234)                              # 난수 고정 -->
<!-- ggraph(graph_comment, layout = "fr") +      # 레이아웃 -->

<!--   geom_edge_link(color = "gray50",          # 엣지 색깔 -->
<!--                  alpha = 0.5) +             # 엣지 명암 -->

<!--   geom_node_point(color = "lightcoral",     # 노드 색깔 -->
<!--                   size = 5) +               # 노드 크기 -->

<!--   geom_node_text(aes(label = name),         # 텍스트 표시 -->
<!--                  repel = T,                 # 노드밖 표시 -->
<!--                  size = 5,                  # 텍스트 크기 -->
<!--                  family = "nanumgothic") +  # 폰트 -->

<!--   theme_graph()                             # 배경 삭제 -->
<!-- ``` -->

<!-- > [참고] 노트 텍스트의 폰트는 `geom_node_text()`의 `family`를 이용해 별도로 설정해야 합니다. `theme()`으로 그래프의 전체 폰트를 바꾸더라도 노드 텍스트에는 적용되지 않습니다.  -->

<!-- <br> -->

<!-- > [참고] `ggraph()`의 `layout`을 이용해 네트워크를 다양한 모양으로 표현할 수 있습니다. 레이아웃에 따라 네트워크가 어떻게 달라지는지 궁금하다면 아래 페이지를 살펴보세요. -->

<!-- > [- data-imaginist.com/2017/ggraph-introduction-layouts](data-imaginist.com/2017/ggraph-introduction-layouts) -->


<!-- #### 네트워크 그래프 함수 만들기 -->

<!-- 앞으로 네트워크 그래프를 자주 만들게 될 것입니다. 같은 코드를 반복 작성하지 않도록 함수를 만들어 활용하겠습니다.  -->

<!-- ```{r} -->
<!-- word_network <- function(x) { -->
<!--   ggraph(x, layout = "fr") + -->
<!--     geom_edge_link(color = "gray50", -->
<!--                    alpha = 0.5) + -->
<!--     geom_node_point(color = "lightcoral", -->
<!--                     size = 5) + -->
<!--     geom_node_text(aes(label = name), -->
<!--                    repel = T, -->
<!--                    size = 5, -->
<!--                    family = "nanumgothic") + -->
<!--     theme_graph() -->
<!-- } -->
<!-- ``` -->

<!-- 이제 `as_tbl_graph()`로 만든 데이터를 `word_network()`에 적용하면 네트워크 그래프가 만들어집니다. -->

<!-- ```{r eval=F} -->
<!-- set.seed(1234) -->
<!-- word_network(graph_comment) -->
<!-- ``` -->

<!-- > [편집] 결과 생략 표시 -->


<!-- ### 5.2.3 유의어 처리하기 -->

<!-- 출력한 그래프를 보면 `"감독"`, `"봉감독"`, `"봉준호감독"`과 같이 의미가 비슷한 단어가 개별 노드로 되어있어 복잡하고 해석하기 어렵습니다. 이처럼 표현은 다르지만 의미가 비슷한 단어를 **유의어(Synonyms)**라고 합니다. 유의어를 한 단어로 통일하면 데이터의 구조를 간결하게 만들므로 텍스트를 해석하는데 도움이 됩니다. -->

<!-- 네트워크에 단어 간의 관계가 더 분명하게 드러나도록 `comment`에서 비슷한 단어들을 통일한 다음 네트워크 그래프 데이터를 다시 만들겠습니다. 출력 결과를 보면 그래프 구조가 이해하기 편해졌음을 알 수 있습니다. -->

<!-- <!-- ```{r} --> -->
<!-- <!-- # 유의어 처리하기 --> -->
<!-- <!-- comment <- comment %>% --> -->
<!-- <!--   mutate( --> -->
<!-- <!--     word = ifelse(str_detect(word, "감독") & --> -->
<!-- <!--                   !str_detect(word, "감독상"), "봉준호", word), --> -->
<!-- <!--     word = ifelse(word  == "오르다", "올리다", word), --> -->
<!-- <!--     word = ifelse(str_detect(word, "축하"), "축하", word)) --> -->

<!-- <!-- # 단어 동시 출현 빈도 구하기 --> -->
<!-- <!-- pair <- comment %>% --> -->
<!-- <!--   pairwise_count(word, id, sort = T) --> -->

<!-- <!-- # 네트워크 그래프 데이터 만들기 --> -->
<!-- <!-- graph_comment <- pair %>% --> -->
<!-- <!--   filter(n >= 25) %>% --> -->
<!-- <!--   as_tbl_graph() --> -->

<!-- <!-- # 네트워크 그래프 만들기 --> -->
<!-- <!-- set.seed(1234) --> -->
<!-- <!-- word_network(graph_comment) --> -->
<!-- <!-- ``` --> -->


<!-- ```{r} -->
<!-- # 유의어 처리하기 -->
<!-- comment <- comment %>% -->
<!--   mutate(word = ifelse(str_detect(word, "감독") & -->
<!--                       !str_detect(word, "감독상"), "봉준호", word),  -->
<!--          word = ifelse(word == "오르다", "올리다", word), -->
<!--          word = ifelse(str_detect(word, "축하"), "축하", word)) -->

<!-- # 단어 동시 출현 빈도 구하기 -->
<!-- pair <- comment %>% -->
<!--   pairwise_count(word, id, sort = T) -->

<!-- # 네트워크 그래프 데이터 만들기 -->
<!-- graph_comment <- pair %>% -->
<!--   filter(n >= 25) %>% -->
<!--   as_tbl_graph() -->

<!-- # 네트워크 그래프 만들기 -->
<!-- set.seed(1234) -->
<!-- word_network(graph_comment) -->
<!-- ``` -->


<!-- --- -->

<!-- > [편집] 지시선 넣어 설명하기 - 코드에 주석을 달면 지저분해지므로 -->

<!-- ```{r eval=F} -->
<!-- # "감독상"을 제외하고 "감독"이 들어간 모든 단어를 "봉준호"로 수정 -->
<!--     word = ifelse(str_detect(word, "감독") & -->
<!--                   !str_detect(word, "감독상"), "봉준호", word), -->

<!-- # "오르다"를 "올리다"로 수정 -->
<!-- word = ifelse(word  == "오르다", "올리다", word), -->

<!-- # "축하"가 들어간 모든 단어를 "축하"로 수정 -->
<!-- word = ifelse(str_detect(word, "축하"), "축하", word)) -->
<!-- ``` -->

<!-- --- -->



<!-- ### 5.2.4 연결 중심성과 커뮤니티 -->

<!-- 네트워크 그래프는 수많은 단어를 노드로 표현하기 때문에 어떤 단어를 중심으로 해석해야 할지 판단하기 어렵습니다. 이런 경우 그래프에 연결 중심성과 커뮤니티를 표현하면 단어의 관계를 보다 분명하게 파악할 수 있습니다. -->


<!-- #### 연결 중심성이란? -->

<!-- **연결 중심성(Degree Centrality)**은 노드가 다른 노드들과 얼마나 깊게 연결되는지를 나타낸 값입니다. 연결 중심성으로 노드의 크기를 조정하면 어떤 단어를 눈여겨봐야 하는지 판단하기 쉬워집니다. `tidygraph`패키지의 `centrality_degree()`를 이용하면 연결 중심성을 구할 수 있습니다. -->

<!-- ```{r echo=FALSE, out.width = '500px'} -->
<!-- knitr::include_graphics(here::here("img/network_centrality.png")) -->
<!-- ``` -->

<!-- #### 커뮤니티란? -->

<!-- 어떤 단어들은 관계가 가까워 자주 함께 사용되고, 어떤 단어들은 그렇지 않습니다. '단어 간의 관계가 가까워 빈번하게 연결된 노드 집단'을 **커뮤니티(Community)**라고 합니다. 노드를 커뮤니티별로 구분지어 서로 다른 색으로 표현하면 네트워크 구조를 이해하는데 도움이 됩니다.  `tidygraph` 패키지의 `group_infomap()`을 이용하면 노드들을 몇 개의 커뮤니티로 구분할 수 있습니다. -->

<!-- ```{r echo=FALSE, out.width = '500px'} -->
<!-- knitr::include_graphics(here::here("img/network_community.png")) -->
<!-- ``` -->

<!-- #### 1. 네트워크 그래프 데이터에 연결 중심성, 커뮤니티 변수 추가하기 -->

<!-- 네트워크 그래프에 연결 중심성과 커뮤니티를 표현하면 단어 동시 출현 빈도를 담은 `pair`를 이용해 네트워크 그래프 데이터를 만든 다음, 연결 중심성과 커뮤니티를 나타낸 변수를 추가하면 됩니다. -->

<!-- **네트워크 그래프 데이터 만들기** : `pair`에서 25회 이상 함께 사용된 단어만 추출하고 `as_tbl_graph()`를 이용해 네트워크 그래프 데이터를 만들겠습니다. `group_infomap()`은 방향성이 없는 네트워크에서만 커뮤니티를 찾아주기 때문에 `as_tbl_graph()`에 `directed = F`를 입력해야합니다. -->

<!-- **연결 중심성, 커뮤니티 변수 추가하기** : `mutate()`에 `centrality_degree()`를 적용해 연결 중심성을 나타낸 변수를 추가하고, `group_infomap()`을 적용해 노드들을 몇 개의 커뮤니티로 분류한 변수를 추가하겠습니다. -->

<!--   `group_infomap()`은 커뮤니티를 정수형 숫자로 표현합니다. 변수가 숫자인 상태로 그래프를 만들면 노드가 그룹에 따라 다른 색으로 표현되는 게 아니라 숫자 크기에 따라 농도가 다른 그라데이션으로 표현됩니다. `as.factor()`를 이용해 `group_infomap()`의 결과를 factor  타입으로 변환하면 이런 현상을 피할 수 있습니다. -->

<!-- `graph_comment`를 출력하면 각 노드에 중심성(centrality)과 커뮤니티(group)가 추가된 것을 확인할 수 있습니다. -->

<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- graph_comment <- pair %>% -->
<!--   filter(n >= 25) %>% -->
<!--   as_tbl_graph(directed = F) %>% -->
<!--   mutate(centrality = centrality_degree(),        # 연결 중심성 -->
<!--          group = as.factor(group_infomap()))      # 커뮤니티 -->

<!-- graph_comment -->
<!-- ``` -->

<!-- > [참고] `group_infomap()`은 그룹을 생성할 때 난수를 이용하기 때문에 코드를 실행할 때마다 그룹이 조금씩 바뀝니다. `set.seed()`로 난수를 고정하면 항상 같은 그룹을 생성합니다. -->

<!-- #### 2. 네트워크 그래프에 연결 중심성, 커뮤니티 표현하기 -->

<!-- `graph_comment`를 이용해 그래프를 만들겠습니다. `geom_node_point()`에 `aes()`를 추가한 다음 연결 중심성에 따라 노드 크기를 결정하도록  `size = centrality`를 입력하고, 커뮤니티에 따라 노드 색깔을 결정하도록 `color = group`를 입력하겠습니다. 그런 다음 `show.legend = F`를 추가해 범례를 표현하지 않도록 하겠습니다. -->

<!-- 노드 크기를 연결 중심성에 따라 결정하면 노드가 너무 크거나 너무 작아서 알아보기 불편합니다. 노드 크기를 5~15 범위로 유지하도록 `scale_size(range = c(5, 15))`를 추가하겠습니다. -->

<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- ggraph(graph_comment, layout = "fr") +      # 레이아웃 -->

<!--   geom_edge_link(color = "gray50",          # 엣지 색깔 -->
<!--                  alpha = 0.5) +             # 엣지 명암 -->

<!--   geom_node_point(aes(size = centrality,    # 노드 크기 -->
<!--                       color = group),       # 노드 색깔 -->
<!--                   show.legend = F) +        # 범례 삭제 -->
<!--   scale_size(range = c(5, 15)) +            # 노드 크기 범위 -->

<!--   geom_node_text(aes(label = name),         # 텍스트 표시 -->
<!--                  repel = T,                 # 노드밖 표시 -->
<!--                  size = 5,                  # 텍스트 크기 -->
<!--                  family = "nanumgothic") +  # 폰트 -->

<!--   theme_graph()                             # 배경 삭제 -->
<!-- ``` -->




<!-- #### 3. 주요 단어 살펴보기 -->

<!-- 네트워크 그래프를 보면 텍스트에서 어떤 단어가 중요한지, 단어가 어떤 맥락에서 사용되었는지 이해할 수 있습니다. -->

<!-- #### 관심 단어의 커뮤니티 살펴보기 -->

<!-- 가장 눈에 띄는 단어는 노드의 크기가 가장 큰 `"봉준호"`입니다. `graph_comment`에서 `name`이 `"봉준호"`인 행을 출력하면 4번 커뮤니티로 분류되어 있음을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- graph_comment %>% -->
<!--   filter(name == "봉준호") -->
<!-- ``` -->

<!-- #### 같은 커뮤니티로 분류된 단어 살펴보기 -->

<!-- `graph_comment`에서 `group`이 `4`인 노드를 추출해 데이터 프레임으로 변환하면 `"봉준호"`가 어떤 단어들과 같은 커뮤니티로 분류되었는지 알 수 있습니다. -->

<!-- ```{r} -->
<!-- graph_comment %>% -->
<!--   filter(group == 4) %>% -->
<!--   arrange(-centrality) %>% -->
<!--   data.frame() -->
<!-- ``` -->

<!-- #### 연결 중심성이 높은 주요 단어 살펴보기 -->

<!-- `graph_comment`를 연결 중심성이 높은 순으로 출력하면 어떤 단어를 눈여겨 봐야할지 알 수 있습니다. 아래 출력 결과를 보면 `"축하"`가 `"봉준호"` 다음으로 연결 중심성이 높고, 2번 커뮤니티로 분류되어 있다는 것을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- graph_comment %>% -->
<!--   arrange(-centrality) -->
<!-- ``` -->

<!-- 2번 커뮤니티로 분류된 단어들을 살펴보면 주로 봉준호 감독의 수상을 축하하는 내용으로 구성되어 있음을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- graph_comment %>% -->
<!--   filter(group == 2) %>% -->
<!--   arrange(-centrality) %>% -->
<!--   data.frame() -->
<!-- ``` -->


<!-- #### 4. 주요 단어가 사용된 원문 살펴보기 -->

<!-- 네트워크에 표현된 단어만 봐서는 맥락을 구체적으로 이해하기 어려울 수 있습니다. 이럴 때 단어가 사용된 텍스트 원문을 함께 살펴보면 맥락을 이해하는데 도움이 됩니다. `filter()`와 `str_detact()`를 이용해 댓글 원문이 들어있는 `news_comment`에서 특정 단어가 사용된 댓글을 추출해 보겠습니다. -->

<!-- ```{r eval=F} -->
<!-- news_comment %>% -->
<!--   filter(str_detect(reply, "봉준호") & str_detect(reply, "대박")) %>% -->
<!--   select(reply) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- news_comment %>% -->
<!--   filter(str_detect(reply, "봉준호") & str_detect(reply, "대박")) %>% -->
<!--   select(reply) %>% -->
<!--   print(n = 3) -->
<!-- ``` -->

<!-- <br> -->

<!-- ```{r eval=F} -->
<!-- news_comment %>% -->
<!--   filter(str_detect(reply, "박근혜") & str_detect(reply, "블랙리스트")) %>% -->
<!--   select(reply) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- news_comment %>% -->
<!--   filter(str_detect(reply, "박근혜") & str_detect(reply, "블랙리스트")) %>% -->
<!--   select(reply) %>% -->
<!--   print(n = 3) -->
<!-- ``` -->

<!-- <br> -->

<!-- ```{r eval=F} -->
<!-- news_comment %>% -->
<!--   filter(str_detect(reply, "기생충") & str_detect(reply, "조국")) %>% -->
<!--   select(reply) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- news_comment %>% -->
<!--   filter(str_detect(reply, "기생충") & str_detect(reply, "조국")) %>% -->
<!--   select(reply) %>% -->
<!--   print(n = 3) -->
<!-- ``` -->


<!-- > [참고] `tidygraph` 패키지에는 `centrality_degree()` 외에도 연결 중심성 지표를 구하는 다양한 함수가 있습니다. 또한 `group_infomap()` 외에도 커뮤니티 탐지 알고리즘을 이용하는 다양한 함수가 있습니다. `tidygraph` 패키지에서 사용할 수 있는 지표와 알고리즘이 궁금하다면 아래 페이지를 살펴보세요. -->

<!-- > [- tidygraph.data-imaginist.com](tidygraph.data-imaginist.com) -->


<!-- ## 5.3 단어 간 상관 분석 - Phi coefficient -->

<!-- 앞에서 보았듯이 `"영화"`와 `"기생충"`은 가장 빈번하게 함께 사용되는 단어쌍입니다. 하지만 이 단어들은 독립적으로도 흔하게 사용되기 때문에 텍스트를 이해하는데 별 다른 도움이 되지 않습니다. 단어의 관계를 분석할 때는 단순히 자주 함께 사용된 단어가 아니라 다른 단어에 비해 상대적으로 자주 함께 사용된 단어가 무엇인지 살펴봐야 합니다. -->

<!-- ### 5.3.1 파이 계수의 개념 알아보기 -->

<!-- **파이 계수(Phi coefficient)**는 두 단어가 함께 사용되는 경우가 각각 사용되는 경우에 비해 얼마나 많은지 나타낸 지표입니다. 파이 계수를 이용하면 어떤 단어와 자주 함께 사용되지만 다른 단어와는 자주 함께 사용되지 않는 단어 즉, 상대적으로 관련성이 큰 단어가 무엇인지 알 수 있습니다. -->


<!-- #### 파이 계수의 의미 -->

<!-- 파이 계수가 어떤 의미를 지니는지 조금 더 자세히 알아보겠습니다. X와 Y라는 두 단어가 있을 때, 여러 텍스트에서 두 단어의 사용 여부를 놓고 가능한 모든 경우를 따져보면 다음과 같이 나누어 볼 수 있습니다. -->

<!-- - X, Y 모두 있음($a$) -->
<!-- - X, Y 모두 없음($d$) -->
<!-- - X만 있음($b$) -->
<!-- - Y만 있음($c$) -->

<!-- 이를 표로 정리하면 다음과 같습니다. -->

<!-- |              | 단어 Y 있음     | 단어 Y 없음     | 전체           |   | -->
<!-- |:------------:|:---------------:|:---------------:|:--------------:|---| -->
<!-- | 단어 X 있음  | $a$             | $b$             | $a+b$          |   | -->
<!-- | 단어 X 없음  | $c$             | $d$             | $c+d$          |   | -->
<!-- | 전체         | $a+c$           | $b+d$           | n              |   | -->

<!-- 각각의 경우에 해당하는 텍스트의 개수를 구한 다음 아래 공식을 이용하면 파이 계수를 계산할 수 있습니다. -->

<!-- $$\phi=\frac{ad-bc}{\sqrt{(a+b)(c+d)(a+c)(b+d)}}$$ -->

<!-- 파이 계수의 범위는 -1 ~ +1입니다. +1에 가까울수록 두 단어가 자주 함께 사용돼 관련성이 크다는 것을 의미합니다. 반대로 -1에 가까울수록 함께 사용되는 경우가 드물어 관련성이 작다는 것을 의미합니다. -->

<!-- #### 5.3.2 파이 계수 구하기 - `pairwise_cor()` -->

<!-- 텍스트를 형태소로 토큰화한 후 **5.2.3**에서 유의어 처리를 완료한 `comment`를 이용해 파이 계수를 구하겠습니다. -->

<!-- 먼저 `add_count()`를 이용해 단어 빈도를 추가하고, 사용 빈도가 낮은 단어 간의 파이 계수는 텍스트의 전반적인 경향을 이해하는 데 도움이 되지 않으므로 20회 이상 사용된 단어만 추출하겠습니다. 그런 다음 `widyr` 패키지의 `pairwise_cor()`를 이용해 파이 계수를 구하겠습니다. `pairwise_cor()`에는 두 가지 파라미터를 지정해야 합니다. -->

<!-- - `item` : 단어가 들어있는 변수. 여기서는 `word`를 입력하면 됩니다. -->
<!-- - `feature` : 텍스트를 구분하는 기준이 되는 변수. 여기서는 `id`를 입력하면 됩니다. -->

<!-- `pairwise_cor()`에 `sort = T`를 추가하면 파이 계수가 높은 순으로 데이터를 정렬하므로 관련성이 큰 단어 중심으로 결과물을 살펴볼 수 있습니다. 출력 결과에서 `correlation`이 단어쌍의 파이 계수를 의미합니다. -->

<!-- ```{r} -->
<!-- word_cors <- comment %>% -->
<!--   add_count(word) %>% -->
<!--   filter(n >= 20) %>% -->
<!--   pairwise_cor(item = word, feature = id, sort = T) -->

<!-- word_cors -->
<!-- ``` -->

<!-- > [참고] `count()`를 이용하면 빈도만 남지만 `add_count()`를 이용하면 원자료를 유지한 상태로 빈도를 나타낸 변수를 추가합니다. -->


<!-- ### 5.3.3 특정 단어와 관련성이 큰 단어 살펴보기 -->

<!-- `filter()`를 이용하면 특정 단어와 관련성이 큰 단어를 살펴볼 수 있습니다. 출력 결과를 보면 단순히 자주 사용된 단어가 아니라 상대적으로 자주 함께 사용된 단어가 무엇인지 알 수 있습니다. -->

<!-- ```{r} -->
<!-- word_cors %>%  -->
<!--   filter(item1 == "대한민국") -->

<!-- word_cors %>%  -->
<!--   filter(item1 == "역사") -->
<!-- ``` -->

<!-- > [편집] 2단 편집 -->

<!-- #### 5.3.4 파이 계수로 막대 그래프 만들기 -->

<!-- 파이 계수로 막대 그래프를 만들면 특정 단어가 어떤 단어와 관련성이 높은지 한 눈에 살펴볼 수 있습니다. -->

<!-- #### 1. 관심 단어별로 파이 계수가 큰 단어 추출하기 -->

<!-- `word_cors`에서 관심 단어별로 파이 계수가 가장 큰 단어를 8개씩 추출하겠습니다. -->

<!-- ```{r} -->
<!-- # 관심 단어 목록 생성 -->
<!-- target <- c("대한민국", "역사", "수상소감", "조국", "박근혜", "블랙리스트") -->

<!-- top_cors <- word_cors %>% -->
<!--   filter(item1 %in% target) %>% -->
<!--   group_by(item1) %>% -->
<!--   slice_max(correlation, n = 8) -->
<!-- ``` -->


<!-- #### 2. 막대 그래프 만들기 -->

<!-- `top_cors`를 이용해 그래프를 만들겠습니다. 출력된 그래프를 보면 관심 단어가 어떤 단어와 관련성이 큰지 한 눈에 알 수 있습니다. -->

<!-- ```{r} -->
<!-- # 그래프 순서 정하기 -->
<!-- top_cors$item1 <- factor(top_cors$item1, levels = target) -->

<!-- library(ggplot2) -->
<!-- ggplot(top_cors, aes(x = reorder_within(item2, correlation, item1), -->
<!--                  y = correlation, -->
<!--                  fill = item1)) + -->
<!--   geom_col(show.legend = F) + -->
<!--   facet_wrap(~ item1, scales = "free") + -->
<!--   coord_flip() + -->
<!--   scale_x_reordered() + -->
<!--   labs(x = NULL) + -->
<!--   theme(text = element_text(family = "nanumgothic")) -->
<!-- ``` -->


<!-- ### 5.3.5 파이 계수로 네트워크 그래프 만들기 -->

<!-- 단어의 동시 출현 빈도를 이용해 네트워크 그래프를 만들었던 것과 같은 방식으로 파이 계수를 이용해 네트워크 그래프를 만들 수 있습니다. 파이 계수로 네트워크 그래프를 만들면 관련성이 큰 단어 중심으로 텍스트의 맥락을 살펴볼 수 있습니다. -->


<!-- #### 1. 네트워크 그래프 데이터 만들고 연결 중심성과 커뮤니티 추가하기 -->

<!-- 네트워크 그래프가 너무 복잡하지 않고 관련성이 큰 단어 중심으로 표현되도록 `word_cors`에서 `correlation`이 `0.15` 이상인 단어쌍을 추출하겠습니다. 그런 다음, 네트워크 그래프 데이터를 만들고, 연결 중심성과 커뮤니티를 추가하겠습니다. -->
<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- graph_cors <- word_cors %>% -->
<!--   filter(correlation >= 0.15) %>% -->
<!--   as_tbl_graph(directed = F) %>% -->
<!--   mutate(centrality = centrality_degree(), -->
<!--          group = as.factor(group_infomap())) -->
<!-- ``` -->

<!-- > [편집] 파이 계수가 몇 이상인 단어쌍을 추출해야 하는지는 정답이 없습니다. 값을 바꿔가며 여러 번 그래프를 만들어 본 다음 적당한 값을 찾아야 합니다. -->

<!-- #### 2. 네트워크 그래프 만들기 -->

<!-- `graph_cors`를 이용해 네트워크 그래프를 만들겠습니다. 단어 간 관련성이 클수록 엣지가 진하게 표현되도록 `geom_edge_link()`에 `aes(edge_alpha = correlation))`를 입력하고, 단어 간 관련성이 클수록 엣지가 두껍게 표현되도록 `edge_width = correlation`를 입력하겠습니다. 출력된 그래프를 보면 단어들이 얼마나 관련성이 큰지에 따라 네트워크를 형성하고 있음을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- ggraph(graph_cors, layout = "fr") + -->

<!--   geom_edge_link(color = "gray50", -->
<!--                  aes(edge_alpha = correlation,   # 엣지 명암 -->
<!--                      edge_width = correlation),  # 엣지 두께 -->
<!--                  show.legend = F) +              # 범례 삭제 -->
<!--   scale_edge_width(range = c(1, 4)) +            # 엣지 두께 범위 -->

<!--   geom_node_point(aes(size = centrality, -->
<!--                       color = group), -->
<!--                   show.legend = F) + -->
<!--   scale_size(range = c(5, 10)) + -->

<!--   geom_node_text(aes(label = name), -->
<!--                  repel = T, -->
<!--                  size = 5, -->
<!--                  family = "nanumgothic") + -->

<!--   theme_graph() -->
<!-- ``` -->


<!-- #### 동시 출현 빈도, 파이 계수 네트워크 그래프의 차이점 -->

<!-- 동시 출현 빈도를 이용한 네트워크 그래프는 `"봉준호"`, `"영화"`와 같이 여러 단어와 자주 함께 사용된 단어 중심으로 네트워크를 형성했습니다. 반면 파이 계수를 이용한 네트워크 그래프는 다른 단어에 비해 상대적으로 자주 함께 사용된, 관련성이 큰 단어 중심으로 네트워크를 형성한다는 특징이 있습니다. -->

<!-- 동시 출현 빈도를 이용한 그래프는 대부분의 노드가 서로 연결되어 있어 구조가 복잡하고 단어 군집이 명확하게 드러나지 않습니다. 반면에 파이 계수를 이용한 그래프는 파이 계수가 특정 값 이상인 단어쌍만 추출해 만들었기 때문에, 관련성이 작은 단어 노드들이 연결되지 않아 단어 군집이 명확하게 드러난다는 특징이 있습니다. -->

<!-- 자주 사용된 단어들이 어떤 맥락에서 사용됐는지 파악하려면 동시 출현 빈도를 활용해 네트워크 그래프를 만드는 게 좋고, 어떤 단어들이 밀접하게 관련되어 있는지 알아보려면 파이 계수를 활용해 네트워크 그래프를 만드는 게 좋습니다. -->


<!-- ## 5.4 연이어 사용된 단어쌍 분석하기 - n-gram -->

<!-- 같은 단어도 앞뒤에 함께 사용된 단어에 따라 의미가 달라질 수 있습니다. 예를 들어 '사과를 먹다'와 '사과를 하다'에서 '사과'는 뒤에 오는 단어인 '먹다', '하다'에 따라 과일이 되기도 하고 미안한 마음을 표현하는 행위가 되기도 합니다. '감을 잡다' 혹은 '귀가 얇다'와 같이 어떤 단어는 연결되어 있을 때만 의미를 갖기도 합니다. -->

<!-- 앞에서 다룬 단어 동시 출현 빈도 분석은 단어들이 텍스트에서 얼마나 자주 함께 사용됐는지를 보고 관계가 있는지 살펴보기 때문에 단어가 연결될 때 생기는 의미를 무시합니다. 단어가 문장에 함께 사용되기만 하면 관련이 있는 것으로 가정하기 때문에 대다수의 텍스트에 사용된 일반적이고 특징 없는 단어를 중점적으로 살펴보게 되는 한계도 있습니다. -->


<!-- #### 엔그램이란? -->

<!-- 연이어 사용된 n개의 단어를 **엔그램(n-gram)**이라고 합니다.  엔그램(n-gram)에서 엔(n)은 몇 개의 단어가 연속되는지를 의미합니다. 두 단어가 연속되면 **바이그램(Bigram)** 혹은 **2-gram**, 세 단어가 연속되면 **트라이그램(Trigram)** 혹은 **3-gram** 이라 부릅니다. -->

<!-- 텍스트를 엔그램으로 토큰화해 분석하면 단어 앞뒤에 어떤 단어가 연이어 사용됐는지 함께 살펴보기 때문에 단어가 연결될 때 생기는 의미와 맥락을 더 잘 이해할 수 있습니다.  엔그램을 이용하면 단어들이 텍스트에 얼마나 자주 '함께' 사용됐는지가 아니라 얼마나 자주 '연이어' 사용됐는지를 중심으로 관계를 살펴보기 때문에 대다수의 텍스트에 사용된 일반적인 단어에 집중하는 문제를 피할 수 있다는 장점도 있습니다. -->

<!-- > [편집] 엔그램 설명 이미지 삽입 -->


<!-- ### 5.4.1 엔그램으로 토큰화하기 -->

<!-- `unnest_tokens()`를 이용하면 텍스트를 엔그램으로 토큰화할 수 있습니다. -->


<!-- #### 샘플 텍스트로 엔그램 토큰화해보기 -->

<!-- 샘플 텍스트를 이용해 엔그램으로 토큰화하는 방법을 알아보겠습니다. `unnest_tokens()`의 `token`에 `"ngrams"`를 입력하고, 몇 단어를 기준으로 할 지, 단어 개수를 `n`에 입력하면 됩니다. `n`에 `2`를 입력하면 바이그램, `3`을 입력하면 트라이그램이 됩니다. -->

<!-- ```{r} -->
<!-- text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.") -->

<!-- # 바이그램 토큰화 -->
<!-- text %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = "ngrams", -->
<!--                 n = 2) -->
<!-- ``` -->

<!-- <br> -->

<!-- ```{r} -->
<!-- # 트라이그램 토큰화 -->
<!-- text %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = "ngrams", -->
<!--                 n = 3) -->
<!-- ``` -->

<!-- > [편집] 2단 편집 -->

<!-- 앞 장에서 단어 동시 출현 빈도 분석을 할 때는 각 단어를 하나의 토큰으로 삼았습니다. 이는 `n`이 `1`인 **유니그램(Unigram)** 혹은 **1-gram**으로 토큰화한 것과 같습니다. 다음 코드를 실행하면 두 방식의 결과물이 같다는 것을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- # 단어 기준 토큰화 -->
<!-- text %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = "words") -->
<!-- ``` -->

<!-- <br> -->

<!-- ```{r} -->
<!-- # 유니그램 토큰화 -->
<!-- text %>% -->
<!--   unnest_tokens(input = value, -->
<!--                 output = word, -->
<!--                 token = "ngrams", -->
<!--                 n = 1) -->
<!-- ``` -->

<!-- > [편집] 2단 편집 -->


<!-- <!-- ngram 오류 수정 --> -->

<!-- <!-- 기존 버전 --> -->
<!-- <!-- 143쪽에서 댓글을 형태소로 토큰화하고 유의어 처리를 완료한 comment를 이용해 바이그램 구함 --> -->
<!-- <!-- comment가 명사와 동사, 형용사를 따로 추출해 구한 다음 합친 방식이라 --> -->
<!-- <!-- 단어 순서가 문장에 사용된 순서가 아니라 앞에는 명사, 뒤에는 동사/형용사과 나열된 형태임.  --> -->
<!-- <!-- 문장에 사용된 순서로 바꿔야함 --> -->


<!-- #### 기사 댓글로 바이그램 만들기 -->

<!-- 영화 ‘기생충’ 관련 기사 댓글을 엔그램으로 토큰화해 분석하겠습니다. 132쪽에서 댓글을 형태소로 토큰화하고 품사별로 행을 분리한 `comment_pos`를 이용해 바이그램을 만들겠습니다. -->


<!-- #### (1) 명사, 동사, 형용사 추출하기 -->

<!-- `comment_pos`에서 명사, 동사, 형용사를 추출해 결합한 후 두 글자 이상만 남깁니다. -->

<!-- ```{r} -->
<!-- comment_new <- comment_pos %>% -->
<!--   separate_rows(word, sep = "[+]") %>% -->
<!--   filter(str_detect(word, "/n|/pv|/pa")) %>% -->
<!--   mutate(word = ifelse(str_detect(word, "/pv|/pa"), -->
<!--                        str_replace(word, "/.*$", "다"), -->
<!--                        str_remove(word, "/.*$"))) %>% -->
<!--   filter(str_count(word) >= 2) %>% -->
<!--   arrange(id) -->
<!-- ``` -->

<!-- > [참고] 텍스트 원문을 바이그램으로 바로 토큰화하면 ‘하다’, ‘했다’, ‘하며’, ‘하므로’ 처럼 원형이 같지만 표현만 다른 단어들이 전부 개별 단어로 취급됩니다. 단어의 표현이 아니라 의미 중심으로 분석해야 하므로 원문에서 형태소를 추출한 다음 바이그램으로 토큰화해야 합니다. -->

<!-- #### (2) 유의어 처리하기 -->

<!-- 143쪽과 마찬가지로 표현은 다르지만 의미가 비슷한 단어를 한 단어로 통일합니다. -->

<!-- ```{r} -->
<!-- comment_new <- comment_new %>% -->
<!--   mutate(word = ifelse(str_detect(word, "감독") & -->
<!--                       !str_detect(word, "감독상"), "봉준호", word),  -->
<!--          word = ifelse(word  == "오르다", "올리다", word), -->
<!--          word = ifelse(str_detect(word, "축하"), "축하", word)) -->
<!-- ``` -->


<!-- #### (3) 한 댓글이 하나의 행이 되도록 결합하기 -->

<!-- `comment_new`의 `word`는 `reply`에서 추출한 개별 단어로 구성되어 있습니다.  -->

<!-- ```{r eval=F} -->
<!-- comment_new %>% -->
<!--   select(word) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- comment_new %>% -->
<!--   select(word) %>% -->
<!--   print(n = 5) -->
<!-- ``` -->

<!-- 한 댓글이 하나의 행이 되도록 `id`별로 `word`를 결합합니다. -->

<!-- ```{r eval=F} -->
<!-- line_comment <- comment_new %>% -->
<!--   group_by(id) %>% -->
<!--   summarise(sentence = paste(word, collapse = " ")) -->

<!-- line_comment -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- line_comment <- comment_new %>% -->
<!--   group_by(id) %>% -->
<!--   summarise(sentence = paste(word, collapse = " ")) -->

<!-- line_comment %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- #### (4) 바이그램으로 토큰화하기 -->

<!-- `unnest_tokens()`를 이용해 `line_comment`를 바이그램으로 토큰화합니다.  -->

<!-- ```{r eval=F} -->
<!-- bigram_comment <- line_comment %>% -->
<!--   unnest_tokens(input = sentence, -->
<!--                 output = bigram, -->
<!--                 token = "ngrams", -->
<!--                 n = 2) -->

<!-- bigram_comment -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- bigram_comment <- line_comment %>% -->
<!--   unnest_tokens(input = sentence, -->
<!--                 output = bigram, -->
<!--                 token = "ngrams", -->
<!--                 n = 2) -->

<!-- bigram_comment %>% -->
<!--   print(n = 5) -->
<!-- ``` -->




<!-- ### 5.4.2 연이어 사용된 단어쌍 빈도 구하기 -->

<!-- 댓글에 어떤 단어쌍이 자주 사용되었는지 알아보겠습니다. -->

<!-- #### 1. 바이그램 분리하기 - `separate()` -->

<!-- `tidyr` 패키지의 `separate()`를 이용해서 바이그램을 구성하는 두 단어를 분리해 서로 다른 변수에 할당합니다. -->

<!-- ```{r eval=F} -->
<!-- # 바이그램 분리하기 -->
<!-- bigram_seprated <- bigram_comment %>% -->
<!--   separate(bigram, c("word1", "word2"), sep = " ") -->

<!-- bigram_seprated -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 바이그램 분리하기 -->
<!-- bigram_seprated <- bigram_comment %>% -->
<!--   separate(bigram, c("word1", "word2"), sep = " ") -->

<!-- bigram_seprated %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- #### 2. 단어쌍 빈도 구하기 -->

<!-- `count()`로 단어쌍의 빈도를 구하고, `na.omit()`을 이용해 결측치가 있는 행을 제거합니다. ‘축하합니다’, ‘멋집니다’ 처럼 한 단어로 된 문장은 바이그램으로 토큰화하면 `NA`가 됩니다. -->


<!-- ```{r eval=F} -->
<!-- # 단어쌍 빈도 구하기 -->
<!-- pair_bigram <- bigram_seprated %>% -->
<!--   count(word1, word2, sort = T) %>% -->
<!--   na.omit() -->

<!-- pair_bigram -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 단어쌍 빈도 구하기 -->
<!-- pair_bigram <- bigram_seprated %>% -->
<!--   count(word1, word2, sort = T) %>% -->
<!--   na.omit() -->

<!-- pair_bigram %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- #### 3. 단어쌍 살펴보기 -->

<!-- 동시 출현 단어쌍을 담은 `pair`와 바이그램 단어쌍을 담은 `pair_bigram`을 비교해 보겠습니다. 동시 출현 단어쌍은 일반적으로 자주 사용된 단어로 구성되며 단어쌍의 종류가 많고 빈도도 높습니다. 반면, 바이그램 단어쌍은 의미가 연결되는 단어로 구성되며 단어쌍의 종류가 적고 빈도도 낮습니다. -->

<!-- ```{r eval=F} -->
<!-- # 동시 출현 단어쌍 -->
<!-- pair %>% -->
<!--   filter(item1 == "대한민국") -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 동시 출현 단어쌍 -->
<!-- pair %>% -->
<!--   filter(item1 == "대한민국") %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- # 바이그램 단어쌍 -->
<!-- pair_bigram %>% -->
<!--   filter(word1 == "대한민국") -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 바이그램 단어쌍 -->
<!-- pair_bigram %>% -->
<!--   filter(word1 == "대한민국") %>% -->
<!--   print(n = 5) -->
<!-- ``` -->

<!-- > [편집] 2단 편집 -->


<!-- ```{r eval=F} -->
<!-- # 동시 출현 단어쌍 -->
<!-- pair %>% -->
<!--   filter(item1 == "아카데미") -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 동시 출현 단어쌍 -->
<!-- pair %>% -->
<!--   filter(item1 == "아카데미") %>% -->
<!--   print(n = 5) -->
<!-- ``` -->

<!-- ```{r eval=F} -->
<!-- # 바이그램 단어쌍 -->
<!-- pair_bigram %>% -->
<!--   filter(word1 == "아카데미") -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 바이그램 단어쌍 -->
<!-- pair_bigram %>% -->
<!--   filter(word1 == "아카데미") %>% -->
<!--   print(n = 5) -->

<!-- ``` -->

<!-- > [편집] 2단 편집 -->


<!-- ### 5.4.3 엔그램으로 네트워크 그래프 만들기 -->

<!-- `pair_bigram`을 이용해 네트워크 그래프 데이터를 만든 다음 142쪽에서 만든 `word_network()`를 이용해 네트워크 그래프를 만들겠습니다. -->

<!-- ```{r} -->
<!-- # 네트워크 그래프 데이터 만들기 -->
<!-- graph_bigram <- pair_bigram %>% -->
<!--   filter(n >= 8) %>% -->
<!--   as_tbl_graph() -->

<!-- # 네트워크 그래프 만들기 -->
<!-- set.seed(1234) -->
<!-- word_network(graph_bigram) -->
<!-- ``` -->


<!-- #### 유의어 통일하고 네트워크 그래프 다시 만들기 -->

<!-- 출력한 그래프를 보면 `"대단"`, `"대단하다"` 처럼 의미가 비슷한 단어가 개별 노드로 되어 있어 네트워크가 복잡하고 해석하기 어렵습니다. 단어의 관계가 분명하게 드러나도록 유의어를 통일하고 네트워크 그래프를 다시 만들겠습니다. -->

<!-- - 단어쌍을 담고 있는 `bigram_seprated`에서 비슷한 단어를 통일합니다.  -->
<!-- - 같은 단어가 연속 사용된 단어쌍을 해석하는 것은 의미가 없으므로 제거합니다.  -->
<!-- - 단어쌍 빈도를 구하고 `na.omit()`을 이용해 결측치를 제거합니다. -->

<!-- > [편집] 본문과 목록 간격 가깝게. 목록내 상하 간격 띄우기. -->



<!-- ```{r} -->
<!-- bigram_seprated <- bigram_seprated %>% -->
<!--   mutate(word1 = ifelse(str_detect(word1, "대단"), "대단", word1), -->
<!--          word2 = ifelse(str_detect(word2, "대단"), "대단", word2), -->

<!--          word1 = ifelse(str_detect(word1, "자랑"), "자랑", word1), -->
<!--          word2 = ifelse(str_detect(word2, "자랑"), "자랑", word2), -->

<!--          word1 = ifelse(str_detect(word1, "짝짝짝"), "짝짝짝", word1), -->
<!--          word2 = ifelse(str_detect(word2, "짝짝짝"), "짝짝짝", word2)) %>% -->

<!--   # 같은 단어 연속 제거 -->
<!--   filter(word1 != word2) -->

<!-- # 단어쌍 빈도 구하기 -->
<!-- pair_bigram <- bigram_seprated %>% -->
<!--   count(word1, word2, sort = T) %>% -->
<!--   na.omit() -->
<!-- ``` -->

<!-- --- -->

<!-- > [알아두면 좋아요] 여러 변수의 유의어 한 번에 처리하기 -->

<!-- > `dplyr` 패키지의 `mutate_at()`과 `case_when()`을 이용하면 여러 변수의 유의어를 처리하는 코드를 다음과 같이 작성할 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- bigram_seprated_new <- bigram_seprated %>%  -->
<!--   mutate_at(vars("word1", "word2"),  -->
<!--             ~ case_when( -->
<!--               str_detect(., "대단") ~ "대단", -->
<!--               str_detect(., "자랑") ~ "자랑", -->
<!--               str_detect(., "짝짝짝") ~ "짝짝짝", -->
<!--               T ~ .)) -->
<!-- ``` -->

<!-- --- -->

<!-- `pair_bigram`를 이용해 네트워크 그래프 데이터를 다시 만듭니다. 네트워크가 너무 복잡하지 않도록 8회 이상 사용된 단어쌍만 추출해 네트워크 그래프 데이터를 생성한 다음 연결 중심성과 커뮤니티를 추가합니다. `ggraph()`를 이용해 네트워크 그래프를 만듭니다. -->

<!-- ```{r} -->
<!-- # 네트워크 그래프 데이터 만들기 -->
<!-- set.seed(1234) -->
<!-- graph_bigram <- pair_bigram %>% -->
<!--   filter(n >= 8) %>% -->
<!--   as_tbl_graph(directed = F) %>% -->
<!--   mutate(centrality = centrality_degree(),    # 중심성 -->
<!--          group = as.factor(group_infomap()))  # 커뮤니티 -->

<!-- # 네트워크 그래프 만들기 -->
<!-- set.seed(1234) -->
<!-- ggraph(graph_bigram, layout = "fr") +         # 레이아웃 -->

<!--   geom_edge_link(color = "gray50",            # 엣지 색깔 -->
<!--                  alpha = 0.5) +               # 엣지 명암 -->

<!--   geom_node_point(aes(size = centrality,      # 노드 크기 -->
<!--                       color = group),         # 노드 색깔 -->
<!--                   show.legend = F) +          # 범례 삭제 -->
<!--   scale_size(range = c(4, 8)) +               # 노드 크기 범위 -->

<!--   geom_node_text(aes(label = name),           # 텍스트 표시 -->
<!--                  repel = T,                   # 노드밖 표시 -->
<!--                  size = 5,                    # 텍스트 크기 -->
<!--                  family = "nanumgothic") +    # 폰트 -->

<!--   theme_graph()                               # 배경 삭제 -->
<!-- ``` -->

<!-- 출력한 네트워크 그래프를 보면 자주 연이어 사용된 단어쌍 중심으로 네트워크를 형성하기 때문에 단어의 맥락과 의미를 구체적으로 이해할 수 있습니다. ‘이미경-부회장’, ‘조국-가족’ 처럼 개별 단어의 빈도는 낮지만 자주 연이어 사용되고 함께 사용할 때 분명한 의미를 지니는 단어쌍도 발견할 수 있습니다.  -->

<!-- <!-- 오류 보정 끝 --> -->


<!-- #### 파이 계수, 바이그램 네트워크 그래프의 차이점 -->

<!-- 네트워크 그래프는 단어쌍을 만들 때 사용한 방법에 따라 특징이 달라집니다. 파이 계수를 사용하면 ‘관련성이 큰 단어쌍’ 중심으로 네트워크가 형성되기 때문에 ‘빈도가 낮아도 관련성이 큰 단어’가 주로 표현됩니다. 반면, 바이그램을 사용하면 ‘연이어 자주 사용된 단어쌍’ 중심으로 표현되기 때문에 ‘관련성이 큰 동시에 자주 사용된 단어’가 주로 표현됩니다. 또한, 파이 계수를 사용하면 관련성이 작은 노드들이 연결되지 않아서 단어 군집이 명확하게 드러나지만 단어들의 전반적인 관계를 파악하기는 어렵습니다. 반면, 바이그램을 사용하면 노드가 대부분 연결되기 때문에 군집은 덜 명확하지만 단어들이 전반적으로 어떤 관계를 형성하고 있는지 알 수 있습니다. -->


<!-- #### 어떤 방법으로 네트워크 그래프를 만드는 게 좋을까? -->

<!-- 지금까지 동시 출현 빈도, 파이 계수, 엔그램을 이용해 네트워크 그래프를 만드는 방법을 알아보았습니다. 각 방법은 특징이 다르므로 분석 목적에 맞게 선택해야 합니다. 세 가지 방법을 모두 사용해 분석 결과를 비교하는 것도 텍스트를 다각도로 이해하는 데 도움이 됩니다. -->

<!-- -**동시 출현 빈도**: 자주 사용된 단어 중심으로 단어들의 관계를 표현하려면 동시 출현 빈도를 사용합니다. -->

<!-- <!-- ```{r echo=FALSE, out.width = '480px'} --> -->
<!-- <!-- knitr::include_graphics(here::here("img/network_cooccurrence.png"),) --> -->
<!-- <!-- ``` --> -->

<!-- > [편집] 그림 제목 삽입 : "동시 출현 빈도로 만든 네트워크 그래프" -->

<!-- -**파이 계수**: 단어가 얼마나 자주 사용되었는지 보다는 관련성이 큰 단어쌍이 무엇인지에 관심이 있고 단어 군집을 잘 드러내고 싶다면 파이 계수를 사용합니다. -->

<!-- <!-- ```{r echo=FALSE, out.width = '400px'} --> -->
<!-- <!-- knitr::include_graphics(here::here("img/network_phi.png"),) --> -->
<!-- <!-- ``` --> -->

<!-- > [편집] 그림 제목 삽입 : "파이 계수로 만든 네트워크 그래프" -->

<!-- -**엔그램**: 연이어 사용될 때 의미를 지니는 단어쌍에 관심이 있고 단어들이 전반적으로 어떤 관계를 형성하고 있는지 표현하려면 엔그램을 사용합니다. -->

<!-- <!-- ```{r echo=FALSE, out.width = '400px'} --> -->
<!-- <!-- knitr::include_graphics(here::here("img/network_ngram.png"),) --> -->
<!-- <!-- ``` --> -->

<!-- > [편집] 그림 제목 삽입 : "엔그램으로 만든 네트워크 그래프" -->

