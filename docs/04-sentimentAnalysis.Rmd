---
title: "Do it! 쉽게 배우는 R 텍스트 마이닝 - 04 감정 분석: 어떤 마음으로 글을 썼을까?"
author: "김영우"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "css/custom.css"]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      ratio: '16:10'
      navigation:
        scroll: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, 
        width = 80,
        # width = 70,
        
        max.print = 80,
        tibble.print_max = 40,
        
        tibble.width = 80,
        # tibble.width = 70,
        
        # pillar.min_chars = Inf, # tibble 문자 출력 제한
        servr.interval = 0.01) # Viewer 수정 반영 속도


knitr::opts_chunk$set(cache = T, warning = F, message = F, 
                      dpi = 300, fig.height = 4)
                      # out.width = "100%"

#xaringanExtra::use_tile_view()

library(knitr)
library(icon)
library(here)
```


```{r echo=FALSE}
rm(list = ls())

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()
showtext_opts(dpi = 300) # opts_chunk$set(dpi=300)

# code highlighting
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})


```

class: title0

Do it! 쉽게 배우는 R 텍스트 마이닝

---

<br>

.pull-left[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
```{r, echo=FALSE, out.width="70%", out.height="70%"}
include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/cover.png")
```
]

.pull-right[

<br>
<br>
<br>

`r fontawesome("github")` [github.com/youngwoos/Doit_textmining](https://github.com/youngwoos/Doit_textmining)

`r fontawesome("facebook-square")` [facebook.com/groups/datacommunity](https://facebook.com/groups/datacommunity)

- [네이버책](https://book.naver.com/bookdb/book_detail.nhn?bid=17891971)
  - [yes24](http://bit.ly/3oUuJOB)
  - [알라딘](http://bit.ly/3oXOSDn)
  - [교보문고](https://bit.ly/2LtNOcB)
]

---

class: title0

04 감정 분석: <br> 어떤 마음으로 글을 썼을까?

---

class: title0-2

We'll make

<br-back-20>

```{r, echo=FALSE, out.width="70%", out.height="70%"}
include_graphics("Image/04/04_2_2.png")
```

---

class: title0-2

and

<br-back-40>

```{r, echo=F, out.width="70%", out.height="70%"}
include_graphics("Image/04/04_4_1.png")
```

---

<br>

.large2[.font-jua[목차]]

.large[.font-jua[04-1 감정 사전 활용하기]]([link](#04-1))

.large[.font-jua[04-2 댓글 감정 분석하기]]([link](#04-2))

.large[.font-jua[04-3 감정 범주별 주요 단어 살펴보기]]([link](#04-3))

.large[.font-jua[04-4 감정 사전 수정하기]]([link](#04-4))


---


name: 04-1
class: title1

04-1 감정 사전 활용하기

---

##### 감정 분석(sentiment analysis)
- 텍스트에 어떤 감정이 담겨있는지 분석하는 방법
  - 글쓴이가 어떤 감정을 담아 글을 썼는가?
  - 사람들이 어떤 주제를 긍정적/부정적으로 느끼는가?

```{r, out.width="80%", eval=F}
include_graphics("Image/etc/04_1_dic.png")
```

---

##### 감정 사전
- '감정 단어'와 '감정의 강도를 표현한 숫자'로 구성된 사전
- 사전을 이용해 문장의 단어에 감정 점수를 부여한 다음 합산

--

#### 감정 사전 살펴보기

- KNU 한국어 감성사전
  - 군산대학교 소프트웨어융합공학과에서 개발
  - `word`: 감정 단어 
  - `polarity`: 감정의 강도

`r fontawesome("lightbulb")` KNU 한국어 감성사전 출처: github.com/park1200656/KnuSentiLex


```{r, eval=FALSE}
# 감정 사전 불러오기
library(dplyr)
library(readr)
dic <- read_csv("knu_sentiment_lexicon.csv")
```

```{r, echo=FALSE}
# 감정 사전 불러오기
library(dplyr)
library(readr)
dic <- read_csv("../Data/knu_sentiment_lexicon.csv")
```

---

.pull-left[
```{r}
# 긍정 단어
dic %>%
  filter(polarity == 2) %>%
  arrange(word)
```
]

.pull-right[

```{r}
# 부정 단어
dic %>%
  filter(polarity == -2) %>%
  arrange(word)
```
]


---

##### 감정 단어의 종류 살펴보기

<br10>

- `word`
  - 한 단어로 된 단일어,  둘 이상 단어 결합된 복합어
  - 이모티콘: `^^`, `ㅠㅠ` 


- `polarity`
  -  5가지 정수(+2, +1, 0, -1, -2)
  - `+`: 긍정 단어, `-`: 부정 단어, `0`: 중성 단어

--

.pull-left[
```{r}
dic %>%
  filter(word %in% c("좋은", "나쁜"))
```
]

.pull-right[
```{r}
dic %>%
  filter(word %in% c("기쁜", "슬픈"))
```
]

---

```{r}
# 이모티콘
library(stringr)
dic %>%
  filter(!str_detect(word, "[가-힣]")) %>%
  arrange(word)

```

---
- 총 14,854개 단어

```{r}
dic %>%
  mutate(sentiment = ifelse(polarity >=  1, "pos",
                     ifelse(polarity <= -1, "neg", "neu"))) %>%
  count(sentiment)
```

---


#### 문장의 감정 점수 구하기

##### 1. 단어 기준으로 토큰화하기
```{r}
df <- tibble(sentence = c("디자인 예쁘고 마감도 좋아서 만족스럽다.",
                          "디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다."))
df
```

---
- 텍스트를 단어 기준으로 토큰화: 감정 사전과 동일하게
- `unnest_tokens(drop = F)`
  - 원문 제거하지 않기
  - 단어가 어느 문장에서 추출됐는지 알수 있도록

```{r df, eval=F}
library(tidytext)
df <- df %>%
  unnest_tokens(input = sentence,
                output = word,
                token = "words",
                drop = F)

df
```

---

```{r, ref.label = "df" , echo=FALSE}
```


---


#### 단어에 감정 점수 부여하기

- `dplyr::left_join()`: `word` 기준 감정 사전 결합
- 감정 사전에 없는 단어 `polarity` `NA` → `0` 부여

```{r join, eval=F}

df <- df %>%
  left_join(dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity))

df
```


---

```{r ref.label="join", echo=F}

```

---

##### 3. 문장별로 감정 점수 합산하기

```{r}
score_df <- df %>%
  group_by(sentence) %>%
  summarise(score  = sum(polarity))

score_df
```

---

name: 04-2
class: title1

04-2 댓글 감정 분석하기

---

##### 영화 '기생충' 아카데미상 수상 관련 기사 댓글

#### 기본적인 전처리

- 고유 번호 변수 만들기: 댓글 내용 같아도 구별할 수 있도록
- html 특수 문자 제거하기: 웹에서 만들어진 텍스트는 html 특수 문자 포함되어 내용 알아보기 불편
  - `textclean::replace_html()` : html 태그를 공백으로 바꾸기
  - `stringr::str_squish()`: 중복 공백 제거
- 특수 문자와 두 글자 미만 단어 포함하기: 
  - 감정 사전의 특수 문자, 모음, 자음으로 된 두 글자 미만의 이모티콘 활용


---

```{r eval=F}
# 데이터 불러오기
raw_news_comment <- read_csv("news_comment_parasite.csv")

# 기본적인 전처리
install.packages("textclean")
library(textclean)

news_comment <- raw_news_comment %>%
  mutate(id = row_number(),
         reply = str_squish(replace_html(reply)))

# 데이터 구조 확인
glimpse(news_comment)
```


```{r echo=F, R.options=list(tibble.width = 50)}
# 데이터 불러오기
raw_news_comment <- read_csv("../Data/news_comment_parasite.csv")

# 기본적인 전처리
# install.packages("textclean")
library(textclean)

news_comment <- raw_news_comment %>%
  mutate(id = row_number(),
         reply = str_squish(replace_html(reply)))

# 데이터 구조 확인
glimpse(news_comment)
```


---

#### 단어 기준으로 토큰화하고 감정 점수 부여하기

```{r, token-comment, eval=FALSE}
# 토큰화
word_comment <- news_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words",
                drop = F)

word_comment %>%
  select(word, reply)
```

---

```{r ref.label="token-comment", R.options=list(tibble.width=50), echo=F}
```
---

```{r}
# 감정 점수 부여
word_comment <- word_comment %>%
  left_join(dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity))

word_comment %>%
  select(word, polarity)
```

---

#### 자주 사용된 감정 단어 살펴보기


##### 1. 감정 분류하기

```{r}
word_comment <- word_comment %>%
  mutate(sentiment = ifelse(polarity ==  2, "pos",
                     ifelse(polarity == -2, "neg", "neu")))

word_comment %>%
  count(sentiment)
```

---

#### 2. 막대 그래프 만들기

```{r top10_sentiment, eval=F}
top10_sentiment <- word_comment %>%
  filter(sentiment != "neu") %>%
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10)

top10_sentiment
```

---

```{r ref.label="top10_sentiment", echo=F}
```

---

```{r p-top10_sentiment, fig.show='hide'}
# 막대 그래프 만들기
library(ggplot2)
ggplot(top10_sentiment, aes(x = reorder(word, n),
                            y = n,
                            fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```

`r fontawesome("lightbulb")` `scale_y_continuous()`: 막대-그래프 간격 넓히기. 막대 끝의 빈도 값이 그래프 경계 밖으로 벗어나지 <br> &nbsp;&nbsp;&nbsp;&nbsp;않도록 설정.

---

```{r "p-top10_sentiment", echo=F}
```


---

#### 댓글별 감정 점수 구하고 댓글 살펴보기



##### 1. 댓글별 감정 점수 구하기

- `id`, `reply`별로 분리한 다음 `polarity` 합산
- `id`로 먼저 나누는 이유: 
    - 내용이 같은 댓글이 여럿 있더라도 서로 다른 댓글로 취급
    - `id`별로 먼저 나누지 않으면 내용 같은 댓글 점수 모두 하나로 합산

```{r score_comment, eval=F}
score_comment <- word_comment %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

score_comment %>%
  select(score, reply)
```

---

```{r score_comment, echo=F, R.options=list(tibble.width = 50)}
```

---

##### 2. 감정 점수 높은 댓글 살펴보기

```{r R.options=list(tibble.width = 50)}
# 긍정 댓글
score_comment %>%
  select(score, reply) %>%
  arrange(-score)
```

---

##### 2. 감정 점수 높은 댓글 살펴보기

```{r R.options=list(tibble.width = 50)}
# 부정 댓글
score_comment %>%
  select(score, reply) %>%
  arrange(score)
```

---

#### 감정 경향 살펴보기



##### 1. 감정 점수 빈도 구하기


.pull-left[
```{r eval=F}
score_comment %>%
  count(score)
```
]

.pull-right[
```{r echo=F}
score_comment %>%
  count(score)
```
]

---

##### 2. 감정 분류하고 막대 그래프 만들기

```{r}
# 감정 분류하기
score_comment <- score_comment %>%
  mutate(sentiment = ifelse(score >=  1, "pos",
                     ifelse(score <= -1, "neg", "neu")))
```

```{r}
# 감정 빈도와 비율 구하기
frequency_score <- score_comment %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100)

frequency_score
```

---
```{r, p-frequency_score, fig.show='hide'}
# 막대 그래프 만들기
ggplot(frequency_score, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))
```

`r fontawesome("lightbulb")` `scale_x_discrete()`: x축 순서 정하기

```{r, ref.label="p-frequency_score", echo=F, out.width="70%"}
# 막대 그래프 만들기
ggplot(frequency_score, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3) +
  scale_x_discrete(limits = c("pos", "neu", "neg"))
```


---

##### 3. 비율 누적 막대 그래프 만들기

<br10>

- 샘플 데이터로 비율 누적 막대 그래프 만들기
  - 데이터에 x축, y축, 누적 막대를 표현할 변수 필요

```{r, out.width="50%"}
df <- tibble(contry = c("Korea", "Korea", "Japen", "Japen"),  # 축
             sex = c("M", "F", "M", "F"),                     # 누적 막대
             ratio = c(60, 40, 30, 70))                       # 값
df
```


---

```{r, out.width="70%"}
ggplot(df, aes(x = contry, y = ratio, fill = sex)) + geom_col()
```

---

- `geom_text()`: 막대에 비율 표기
- `position_stack(vjust = 0.5)`: 비율을 막대의 가운데에 표시

```{r, out.width="70%"}
ggplot(df, aes(x = contry, y = ratio, fill = sex)) +
  geom_col() +
  geom_text(aes(label = paste0(ratio, "%")),          # % 표시
            position = position_stack(vjust = 0.5))   # 가운데 표시
```

---

**댓글의 감정 비율로 누적 막대 그래프 만들기**
- x축을 구성할 더미 변수(dummy variable) 추가

```{r}
# 더미 변수 생성
frequency_score$dummy <- 0
frequency_score
```

---

```{r p-frequency_score-ratio, out.width="40%", fig.width=4, fig.height=4}
ggplot(frequency_score, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")),
              position = position_stack(vjust = 0.5)) +
  theme(axis.title.x = element_blank(),  # x축 이름 삭제
        axis.text.x  = element_blank(),  # x축 값 삭제
        axis.ticks.x = element_blank())  # x축 눈금 삭제
```

---
```{r ref.label="p-frequency_score-ratio", echo=F, out.width="60%", fig.width=4, fig.height=4}
```

---

name: 04-3
class: title1

04-3 감정 범주별 주요 단어 살펴보기

---


#### 감정 범주별 단어 빈도 구하기

##### 1. 토큰화하고 두 글자 이상 한글 단어만 남기기

```{r}
comment <- score_comment %>%
  unnest_tokens(input = reply,          # 단어 기준 토큰화
                output = word,
                token = "words",
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") &  # 한글 추출
         str_count(word) >= 2)          # 두 글자 이상 추출
```

---

##### 2. 감정 범주별 빈도 구하기

```{r}
# 감정 및 단어별 빈도 구하기
frequency_word <- comment %>%
  filter(str_count(word) >= 2) %>%
  count(sentiment, word, sort = T)

frequency_word
```

---

.pull-left[
```{r}
# 긍정 댓글 고빈도 단어
frequency_word %>%
  filter(sentiment == "pos")
```
]

.pull-right[
```{r}
# 부정 댓글 고빈도 단어
frequency_word %>%
  filter(sentiment == "neg")
```

]


---

#### 상대적으로 자주 사용된 단어 비교하기

##### 1. 로그 오즈비 구하기

.pull-left[

```{r comment_wide, eval=F}
# wide form으로 변환
library(tidyr)
comment_wide <- frequency_word %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = list(n = 0))

comment_wide
```
]

.pull-right[

```{r ref.label="comment_wide", echo=F}
```

]


---

```{r}
# 로그 오즈비 구하기
comment_wide <- comment_wide %>%
  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /
                              ((neg + 1) / (sum(neg + 1)))))

comment_wide
```


---

##### 2. 로그 오즈비가 가장 큰 단어 10개씩 추출하기

```{r odds-top10, eval=F}
top10 <- comment_wide %>%
  group_by(sentiment = ifelse(log_odds_ratio > 0, "pos", "neg")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

top10
```

`r fontawesome("lightbulb")` 로그 오즈비 동점 단어 제외

---

```{r odds-top10, echo=F}
```

---

##### 3. 막대 그래프 만들기

```{r fig.width=6, fig.height=4, out.width="50%"}
# 막대 그래프 만들기
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                      y = log_odds_ratio,
                      fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```



---

```{r fig.width=6, fig.height=4, out.width="70%", echo=F}
# 막대 그래프 만들기
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                      y = log_odds_ratio,
                      fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```

---

name: 04-4
class: title1

04-4 감정 사전 수정하기

---

#### 감정 단어가 사용된 원문 살펴보기

- `"소름"`, `"미친"`: 긍정적인 감정을 극적으로 표현한 단어

```{r, R.options=list(tibble.width = 50)}
# "소름"이 사용된 댓글
score_comment %>%
  filter(str_detect(reply, "소름")) %>%
  select(reply)
```

---

#### 감정 단어가 사용된 원문 살펴보기

- `"소름"`, `"미친"`: 긍정적인 감정을 극적으로 표현한 단어

```{r, R.options=list(tibble.width = 50, tibble.print_max=10)}
# "미친"이 사용된 댓글
score_comment %>%
  filter(str_detect(reply, "미친")) %>%
  select(reply)
```

---

- `"소름"`, `"미친"`이 감정 사전에 부정적인 단어로 분류되어 있어서 생긴 문제
- 텍스트의 맥락이 감정 사전의 맥락과 다르면 반대되는 감정 점수 부여
- 감정 사전 수정 필요

```{r}
dic %>% filter(word %in% c("소름", "소름이", "미친"))
```


---

#### 감정 사전 수정하기

- `"소름이"`, `"소름"`, `"미친"`의 `polarity`를 양수 2로 수정

```{r}
new_dic <- dic %>%
  mutate(polarity = ifelse(word %in% c("소름", "소름이", "미친"), 2, polarity))

new_dic %>% filter(word %in% c("소름", "소름이", "미친"))
```

---


#### 수정한 사전으로 감정 점수 부여하기

```{r}
new_word_comment <- word_comment %>%
  select(-polarity) %>%
  left_join(new_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity))
```

---

#### 댓글별 감정 점수 구하기

<br-back-10>

```{r, R.options=list(tibble.width = 50)}
new_score_comment <- new_word_comment %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

new_score_comment %>%
  select(score, reply) %>%
  arrange(-score)
```

`r fontawesome("lightbulb")` `ungroup()`: 이후 분석 작업은 그룹별로 처리하지 않도록 그룹 해제

---




#### 전반적인 감정 경향 살펴보기


##### 1. 감정 분류하기

```{r}
# 1점 기준으로 긍정 중립 부정 분류
new_score_comment <- new_score_comment %>%
  mutate(sentiment = ifelse(score >=  1, "pos",
                     ifelse(score <= -1, "neg", "neu")))
```

---

##### 2. 감정 범주별 빈도와 비율 구하기

.pull-left[
```{r}
# 원본 감정 사전 활용
score_comment %>% #<<
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100)
```
]

.pull-right[

```{r}
# 수정한 감정 사전 활용
new_score_comment %>% #<<
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100)
```
]

---

##### 3. 분석 결과 비교하기

```{r}
word <- "소름|소름이|미친"
```


.pull-left[
```{r}
# 원본 감정 사전 활용
score_comment %>% #<<
  filter(str_detect(reply, word)) %>%
  count(sentiment)
```
]

.pull-right[


```{r}
# 수정한 감정 사전 활용
new_score_comment %>% #<<
  filter(str_detect(reply, word)) %>%
  count(sentiment)
```
]

`r icon_style(fontawesome("exclamation-triangle"), fill = "#FF7333")` `str_detect()`에 여러 문자를 입력할 때는 `|`로 문자를 구분

---

#### 감정 범주별 주요 단어 살펴보기

##### 1. 두 글자 이상 한글 단어만 남기고 단어 빈도 구하기

```{r}
# 토큰화 및 전처리
new_comment <- new_score_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words",
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") &
           str_count(word) >= 2)

# 감정 및 단어별 빈도 구하기
new_frequency_word <- new_comment %>%
  count(sentiment, word, sort = T)
```


---

##### 2. 로그 오즈비 구하기

```{r}
# Wide form으로 변환
new_comment_wide <- new_frequency_word %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = list(n = 0))

# 로그 오즈비 구하기
new_comment_wide <- new_comment_wide %>%
  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /
                              ((neg + 1) / (sum(neg + 1)))))
```

---

##### 3. 로그 오즈비가 큰 단어로 막대 그래프 만들기


```{r fig.width=6, fig.height=4, out.width="38%"}
new_top10 <- new_comment_wide %>%
  group_by(sentiment = ifelse(log_odds_ratio > 0, "pos", "neg")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

ggplot(new_top10, aes(x = reorder(word, log_odds_ratio),
                      y = log_odds_ratio,
                      fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```

---

```{r fig.width=6, fig.height=4, out.width="80%", echo=FALSE}
new_top10 <- new_comment_wide %>%
  group_by(sentiment = ifelse(log_odds_ratio > 0, "pos", "neg")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

ggplot(new_top10, aes(x = reorder(word, log_odds_ratio),
                      y = log_odds_ratio,
                      fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))
```

---

##### 4. 주요 단어가 사용된 댓글 살펴보기

##### 긍정 댓글 원문

```{r, R.options=list(tibble.width = 50)}
new_score_comment %>%
  filter(sentiment == "pos" & str_detect(reply, "축하")) %>%
  select(reply)
```

---

##### 4. 주요 단어가 사용된 댓글 살펴보기

##### 긍정 댓글 원문


```{r, R.options=list(tibble.width = 50)}
new_score_comment %>%
  filter(sentiment == "pos" & str_detect(reply, "소름")) %>%
  select(reply)
```

---

##### 4. 주요 단어가 사용된 댓글 살펴보기

##### 부정 댓글 원문

```{r, R.options=list(tibble.width = 50, tibble.print_max = 15)}
new_score_comment %>%
  filter(sentiment == "neg" & str_detect(reply, "좌빨")) %>%
  select(reply)
```

---

##### 4. 주요 단어가 사용된 댓글 살펴보기

##### 부정 댓글 원문

```{r, R.options=list(tibble.width = 50)}
new_score_comment %>%
  filter(sentiment == "neg" & str_detect(reply, "못한")) %>%
  select(reply)
```

---

##### 5. 분석 결과 비교하기

.pull-left[
```{r eval=F}
# 수정한 감정 사전 활용
new_top10 %>% #<<
  select(-pos, -neg) %>%
  arrange(-log_odds_ratio)
```
]

.pull-right[

```{r eval=F}
# 원본 감정 사전 활용
top10 %>% #<<
  select(-pos, -neg) %>%
  arrange(-log_odds_ratio)
```
]

---

.pull-left[
```{r echo=F, highlight.output = c(7)}
# 수정한 감정 사전 활용
new_top10 %>%
  select(-pos, -neg) %>%
  arrange(-log_odds_ratio)
```
]

.pull-right[

```{r echo=F, highlight.output = c(22, 24)}
# 원본 감정 사전 활용
top10 %>%
  select(-pos, -neg) %>%
  arrange(-log_odds_ratio)
```
]

---

- `"미친"`이 목록에서 사라짐: 로그 오즈비가 10위 안에 들지 못할 정도로 낮기 때문
  - 긍정 단어 10위 `"최고의"` 로그 오즈비 2.90

```{r}
new_comment_wide %>% 
  filter(word == "미친")
```


---


class: title1

정리하기

---

### 정리하기

##### 1. 단어 빈도 비교하기

```{r eval=F}
# 토큰화
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

# 하위 집단별 단어 빈도 구하기
frequency <- speeches %>%
  count(president, word) %>%
  filter(str_count(word) > 1)

# 가장 많이 사용된 단어 추출
top10 <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)
```

---

### 정리하기

##### 2. 로그 오즈비로 단어 비교하기

```{r}
# long form을 wide form으로 변환
frequency_wide <- frequency %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0))

# 로그 오즈비 구하기
frequency_wide <- frequency_wide %>%
  mutate(log_odds_ratio = log(((moon + 1) / (sum(moon + 1))) /
                              ((park + 1) / (sum(park + 1)))))

# 상대적으로 중요한 단어 추출
top10 <- frequency_wide %>%
  group_by(president = ifelse(log_odds_ratio > 0, "moon", "park")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)
```

---

### 정리하기

##### 3. TF-IDF로 단어 비교하기

```{r eval=F}
# TF-IDF 구하기
frequecy <- frequecy %>%
  bind_tf_idf(term = word,
              document = president,
              n = n) %>%
  arrange(-tf_idf)

# 상대적으로 중요한 단어 추출
top10 <- frequecy %>%
  arrange(tf_idf) %>%
  group_by(president) %>%
  slice_max(tf_idf, n = 10, with_ties = F)
```

---

class: title1

분석 도전

---

### 분석 도전(1/2)

**Q1. 역대 대통령의 대선 출마 선언문을 담은 `speeches_presidents.csv`를 이용해 문제를 해결해 보세요.**

Q1.1 `speeches_presidents.csv`를 불러와 이명박 전 대통령과 노무현 전 대통령의 연설문을 추출하고 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 분석에 적합하게 전처리하세요.

Q1.2 연설문에서 명사를 추출한 다음 연설문별 단어 빈도를 구하세요.

Q1.3 로그 오즈비를 이용해 두 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

Q1.4 두 연설문에서 상대적으로 중요한 단어를 나타낸 막대 그래프를 만드세요.



---

Q1.1 `speeches_presidents.csv`를 불러와 이명박 전 대통령과 노무현 전 대통령의 연설문을 추출하고 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 분석에 적합하게 전처리하세요.

```{r eval=FALSE}
# 데이터 불러오기
library(readr)
raw_speeches <- read_csv("speeches_presidents.csv")

# 전처리
library(dplyr)
library(stringr)
speeches <- raw_speeches %>%
  filter(president %in% c("이명박", "노무현")) %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

speeches
```

```{r echo=F, R.options=list(tibble.width = 50)}
library(readr)
raw_speeches <- read_csv("../Data/speeches_presidents.csv")

# 전처리
library(dplyr)
library(stringr)
speeches <- raw_speeches %>%
  filter(president %in% c("이명박", "노무현")) %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

speeches
```

---

Q1.2 연설문에서 명사를 추출한 다음 연설문별 단어 빈도를 구하세요.

<br-back-10>

```{r eval=FALSE}
# 명사 추출
library(tidytext)
library(KoNLP)
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

speeches
```

<br-back-10>

```{r echo=FALSE}
# 명사 추출
library(tidytext)
library(KoNLP)
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

speeches
```



---

Q1.2 연설문에서 명사를 추출한 다음 연설문별 단어 빈도를 구하세요.

```{r}
# 연설문별 단어 빈도 구하기
frequency <- speeches %>%
  count(president, word) %>%
  filter(str_count(word) > 1)

frequency
```

---

Q1.3 로그 오즈비를 이용해 두 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

```{r}
# long form을 wide form으로 변환
library(tidyr)
frequency_wide <- frequency %>%
  pivot_wider(names_from = president,     # 변수명으로 만들 값
              values_from = n,            # 변수에 채워 넣을 값
              values_fill = list(n = 0))  # 결측치 0으로 변환

frequency_wide
```

---

Q1.3 로그 오즈비를 이용해 두 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

```{r}
# 로그 오즈비 구하기
frequency_wide <- frequency_wide %>%
  mutate(log_odds_ratio = log(((이명박 + 1) / (sum(이명박 + 1))) /
                              ((노무현 + 1) / (sum(노무현 + 1)))))

frequency_wide
```

---

Q1.3 로그 오즈비를 이용해 두 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

```{r eval=F}
# 상대적으로 중요한 단어 추출
top10 <- frequency_wide %>%
  group_by(president = ifelse(log_odds_ratio > 0, "lee", "roh")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

top10
```

---
```{r echo=FALSE}
# 상대적으로 중요한 단어 추출
top10 <- frequency_wide %>%
  group_by(president = ifelse(log_odds_ratio > 0, "lee", "roh")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

top10
```


---

Q1.4 두 연설문에서 상대적으로 중요한 단어를 나타낸 막대 그래프를 만드세요.
```{r fig.width=6, fig.height=4, out.width="50%"}
library(ggplot2)
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                  y = log_odds_ratio,
                  fill = president)) +
  geom_col() +
  coord_flip () +
  labs(x = NULL)
```

---
```{r echo=F, fig.width=6, fig.height=4, out.width="90%"}
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                  y = log_odds_ratio,
                  fill = president)) +
  geom_col() +
  coord_flip () +
  labs(x = NULL)
```

---


### 분석 도전(2/2)

**Q2. 역대 대통령의 취임사를 담은 `inaugural_address.csv`를 이용해 문제를 해결해 보세요.**

Q2.1 `inaugural_address.csv`를 불러와 분석에 적합하게 전처리하고 연설문에서 명사를 추출하세요.

Q2.2 TF-IDF를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

Q2.3 각 연설문에서 상대적으로 중요한 단어를 나타낸 막대 그래프를 만드세요.

---

Q2.1 `inaugural_address.csv`를 불러와 분석에 적합하게 전처리하고 연설문에서 명사를 추출하세요.

```{r eval=FALSE}
# 데이터 불러오기
library(readr)
raw_speeches <- read_csv("inaugural_address.csv")

# 전처리
library(dplyr)
library(stringr)
speeches <- raw_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

speeches
```

```{r echo=FALSE, R.options=list(tibble.width = 50)}
# 데이터 불러오기
library(readr)
raw_speeches <- read_csv("../Data/inaugural_address.csv")

# 전처리
library(dplyr)
library(stringr)
speeches <- raw_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

speeches
```

---

Q2.1 `inaugural_address.csv`를 불러와 분석에 적합하게 전처리하고 연설문에서 명사를 추출하세요.


```{r eval=F}
# 명사 기준 토큰화
library(tidytext)
library(KoNLP)
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

speeches
```

---
```{r echo=F}
# 명사 기준 토큰화
library(tidytext)
library(KoNLP)
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

speeches
```

---

Q2.2 TF-IDF를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

```{r}
# 단어 빈도 구하기
frequecy <- speeches %>%
  count(president, word) %>%
  filter(str_count(word) > 1)

frequecy
```

---

Q2.2 TF-IDF를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

```{r}
# TF-IDF 구하기
frequecy <- frequecy %>%
  bind_tf_idf(term = word,           # 단어
              document = president,  # 텍스트 구분 변수
              n = n) %>%             # 단어 빈도
  arrange(-tf_idf)

frequecy
```

---

Q2.2 TF-IDF를 이용해 각 연설문에서 상대적으로 중요한 단어를 10개씩 추출하세요.

```{r}
# 상대적으로 중요한 단어 추출
top10 <- frequecy %>%
  group_by(president) %>%
  slice_max(tf_idf, n = 10, with_ties = F)

head(top10)
```


---



Q2.3 각 연설문에서 상대적으로 중요한 단어를 나타낸 막대 그래프를 만드세요.

```{r fig.width=6, fig.height=5, out.width="35%"}
library(ggplot2)
ggplot(top10, aes(x = reorder_within(word, tf_idf, president),
                  y = tf_idf,
                  fill = president)) +
  geom_col(show.legend = F) +
  coord_flip () +
  facet_wrap(~ president, scales = "free", ncol = 2) +
  scale_x_reordered() +
  labs(x = NULL)
```

---


```{r echo=F, fig.width=6, fig.height=5, out.width="75%"}
ggplot(top10, aes(x = reorder_within(word, tf_idf, president),
                  y = tf_idf,
                  fill = president)) +
  geom_col(show.legend = F) +
  coord_flip () +
  facet_wrap(~ president, scales = "free", ncol = 2) +
  scale_x_reordered() +
  labs(x = NULL)
```

---


class: title0

끝
