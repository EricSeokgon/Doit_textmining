---
title: "Do it! 쉽게 배우는 R 텍스트 마이닝 - 06 토픽 모델링: 어떤 주제로 글을 썼을까?"

author: "김영우"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "../css/custom_06.css"]
    lib_dir: libs
    chakra: ../libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      ratio: '16:10'
      navigation:
        scroll: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, 
        width = 80,
        # width = 70,
        
        max.print = 80,
        tibble.print_max = 40,
        
        tibble.width = 80,
        # tibble.width = 70,
        
        # pillar.min_chars = Inf, # tibble 문자 출력 제한
        servr.interval = 0.01) # Viewer 수정 반영 속도


knitr::opts_chunk$set(cache = T, warning = F, message = F, 
                      dpi = 300, fig.height = 4)
                      # out.width = "100%"

# xaringanExtra::use_tile_view()

library(knitr)
library(icon)
library(here)
```


```{r echo=FALSE}
rm(list = ls())

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()
showtext_opts(dpi = 96)
# showtext_opts(dpi = 300) # opts_chunk$set(dpi=300)

# code highlighting
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})

# 출력 결과 제한 함수
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```


class: title0

Do it! 쉽게 배우는 R 텍스트 마이닝

---

class: no-page-num

<br>

.pull-left[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
```{r, echo=FALSE, out.width="70%", out.height="70%"}
include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/cover.png")
```
]

.pull-right[

<br>
<br>
<br>

`r fontawesome("github")` [github.com/youngwoos/Doit_textmining](https://github.com/youngwoos/Doit_textmining)

`r fontawesome("facebook-square")` [facebook.com/groups/datacommunity](https://facebook.com/groups/datacommunity)

- [네이버책](https://book.naver.com/bookdb/book_detail.nhn?bid=17891971)
  - [yes24](http://bit.ly/3oUuJOB)
  - [알라딘](http://bit.ly/3oXOSDn)
  - [교보문고](https://bit.ly/2LtNOcB)
]

---

class: title0

06 토픽 모델링:  
어떤 주제로 글을 썼을까?

---

class: title0-2

<br-back-20>

We'll make

<br-back-40>

```{r, echo=FALSE, out.width="60%", out.height="60%"}
include_graphics("../Image/06/06_4_1.png")
```

---

class: title0-2

<br-back-40>

and

<br-back-40>

```{r, echo=F, out.width="65%", out.height="65%"}
include_graphics("../Image/06/06_5_1_edit.png")
```

---

<br>

.large2[.font-jua[목차]]

.large[.font-jua[06-1 토픽 모델링 개념 알아보기]]([link](#06-1))

.large[.font-jua[06-2 LDA 모델 만들기]]([link](#06-2))

.large[.font-jua[06-3 토픽별 주요 단어 살펴보기]]([link](#06-3))

.large[.font-jua[06-4 문서를 토픽별로 분류하기]]([link](#06-4))

.large[.font-jua[06-5 토픽 이름 짓기]]([link](#06-5))

.large[.font-jua[06-6 최적의 토픽 수 정하기]]([link](#06-6))


---

name: 06-1
class: title1

06-1 토픽 모델링 개념 알아보기

---

#### 토픽 모델링(topic modeling)
  - 텍스트의 핵심 주제를 찾아 비슷한 내용끼리 분류하는 분석 방법
  - 분석할 텍스트가 많을 때 유용
  
```{r echo=FALSE, out.width = '70%'}
knitr::include_graphics("../Image/06/06_3_1.png")
```


---

##### 토픽 모델 예시: 문서 3개로 만든 모델

.pull-left-60[

<br10>

- 문서의 토픽
  - 문서 1: 고양이 관련 내용
  - 문서 2: 음식 관련 내용
  - 문서 3: 고양이, 음식 모두 관련 내용
]


<br10>

.pull-right-40[

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_1.png")
```
]


--


.pull-left-60[
<br>
<br>
<br>

- 토픽 모델을 이용하면
  - 단어가 어떤 토픽에 등장할 확률이 더 높은지 알 수 있다
  - 단어 등장 확률을 보고 토픽의 핵심 단어를 알 수 있다
]

.pull-right-40[
<br>

```{r echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_2.png")
```
]


---

- 토픽 모델을 이용하면
  - 문서가 어떤 토픽에 등장할 확률이 높은지 알 수 있다
  - 확률을 이용해 문서를 토픽별로 분류할 수 있다 → 다량의 문서 분석할 때 특히 유용
  - 문서가 어떤 주제로 구성되는지 파악할 수 있다

<br10>

```{r echo=FALSE, out.width = '25%', fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_3.png")
```


---


#### LDA 모델
- LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당): 가장 널리 사용되는 토픽 모델링 알고리즘


##### LDA 모델의 가정 1. 토픽은 여러 단어의 혼합으로 구성된다

- 한 토픽에 여러 단어가 서로 다른 확률로 포함된다
- 같은 단어가 여러 토픽에 서로 다른 확률로 포함된다

<br10>

```{r echo=FALSE, out.width = '25%', fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_4.png")
```

---

##### LDA 모델의 가정 2. 문서는 토픽들의 혼합으로 구성된다

- 문서에는 여러 토픽의 단어가 서로 다른 비율로 들어 있음
- 단어 확률이 더 높은 쪽으로 문서 분류

<br10>

```{r echo=FALSE, out.width='50%', fig.cap="Latent Dirichlet allocation(LDA): bit.ly/easytext_62", fig.align='center'}
knitr::include_graphics("../Image/06/Blei_ICML_2012_edit.png")
```

<br>

`r fontawesome("lightbulb")` LDA 모델이 만들어지는 과정을 자세히 알고 싶다면 - Topic Modeling, LDA: [bit.ly/easytext_61](https://bit.ly/easytext_61)


---

name: 06-2
class: title1

6.2 LDA 모델 만들기


---

#### 전처리하기

##### 1. 기본적인 전처리

- **중복 문서 제거하기**: `dplyr::distinct()`
  - 중복 문서가 있으면 계산량 늘어나 모델 만드는 시간 오래 걸림
  - 한 토픽에 내용이 똑같은 문서가 여러 개 들어 있는 문제 생김


- **짧은 문서 제거하기**:
  - 토픽 모델은 여러 문서에 공통으로 사용된 단어를 이용해 만듦
  - 짧은 문서는 다른 문서와 공통으로 사용된 단어가 적어 모델 만드는 데 적합하지 않음

---

```{r eval = F}
# 기생충 기사 댓글 불러오기
library(readr)
library(dplyr)

raw_news_comment <- read_csv("news_comment_parasite.csv") %>%
  mutate(id = row_number())

library(stringr)
library(textclean)

# 기본적인 전처리
news_comment <- raw_news_comment %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply)) %>%

  # 중복 댓글 제거
  distinct(reply, .keep_all = T) %>%

  # 짧은 문서 제거 - 3 단어 이상 추출
  filter(str_count(reply, boundary("word") >= 3))
```

```{r echo = F}
# 기생충 기사 댓글 불러오기
library(readr)
library(dplyr)

raw_news_comment <- read_csv("../../Data/news_comment_parasite.csv") %>%
  mutate(id = row_number())

library(stringr)
library(textclean)

# 기본적인 전처리
news_comment <- raw_news_comment %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply)) %>%

  # 중복 댓글 제거
  distinct(reply, .keep_all = T) %>%

  # 짧은 문서 제거 - 3 단어 이상 추출
  filter(str_count(reply, boundary("word")) >= 3)
```

`r fontawesome("lightbulb")` `row_number()`: 문서를 토픽별로 분류하는 작업을 할 때 문서 구분 기준이 필요하므로 댓글 고유 번호 부여

---

##### 2. 명사 추출하기

- 문서의 주제는 명사로 결정되므로 명사 추출해 모델 만드는 경우가 많음
- 댓글에 중복 사용된 단어 제거: 문서에 같은 단어 여러 번 사용되면 내용 관계없이 사용 빈도 때문에 특정 토픽으로 분류될 가능성 높음

```{r}
library(tidytext)
library(KoNLP)

# 명사 추출
comment <- news_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = extractNoun,
                drop = F) %>%
  filter(str_count(word) > 1) %>%

  # 댓글 내 중복 단어 제거
  group_by(id) %>%
  distinct(word, .keep_all = T) %>%
  ungroup() %>%
  select(id, word)
```

---

```{r}
comment
```

---

##### 3. 빈도 높은 단어 제거하기

- '영화', '기생충' 등은 거의 모든 댓글에 들어 있음
- 빈도가 매우 높은 단어가 포함된 상태로 토픽 모델을 만들면 대부분의 토픽에 똑같은 단어가 주요 단어로 등장해 토픽의 특징을 파악하기 어려우므로 제거

```{r}
count_word <- comment %>%
  add_count(word) %>%
  filter(n <= 200) %>%
  select(-n)
```

---

##### 4. 불용어 제거하기, 유의어 처리하기

<br10>

##### 4.1 불용어, 유의어 확인하기
- 불용어(Stop word): 분석에서 제외할 단어
  - `"들이"`, `"하다"`, `"하게"`처럼 의미를 알 수 없는 단어
  - 텍스트 해석에 도움이 되지 않으므로 제거해야 함
  

<br>

- 빈도 높은 단어 추출해 불용어 확인, 표현은 다르지만 의미가 비슷한 유의어가 있는지 확인

```{r eval=F}
# 불용어, 유의어 확인하기
count_word %>%
  count(word, sort = T) %>%
  print(n = 200)
```

---

```{r echo=F, R.options=list(tibble.print_max = 20, tibble.print_min = 20)}
count_word %>%
  count(word, sort = T)
```


---

##### 4.2 불용어 목록 만들기

```{r}
# 불용어 목록 만들기
stopword <- c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
              "해요", "이것", "니들", "하기", "하지", "한거", "해주",
              "그것", "어디", "여기", "까지", "이거", "하신", "만큼")
```

---


##### 4.3 불용어 제거하고 유의어 수정하기

```{r}
# 불용어, 유의어 처리하기
count_word <- count_word %>%
  filter(!word %in% stopword) %>%
  mutate(word = recode(word,
                       "자랑스럽습니" = "자랑",
                       "자랑스럽" = "자랑",
                       "자한" = "자유한국당",
                       "문재" = "문재인",
                       "한국의" = "한국",
                       "그네" = "박근혜",
                       "추카" = "축하",
                       "정경" = "정경심",
                       "방탄" = "방탄소년단"))
```

`r fontawesome("lightbulb")` `dplyr::recode()`: 특정 값을 다른 값으로 수정

---


.box[
<br-back-20>
.info[`r icon_style(fontawesome("rocket"), fill = "#FF7333")` 불용어 목록을 파일로 만들어 활용하기]

<br10>

```{r eval=F}
# tibble 구조로 불용어 목록 만들기
stopword <- tibble(word = c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
                            "해요", "이것", "니들", "하기", "하지", "한거", "해주",
                            "그것", "어디", "여기", "까지", "이거", "하신", "만큼")

# 불용어 목록 저장하기
library(readr)
write_csv(stopword, "stopword.csv")

# 불용어 목록 불러오기
stopword <- read_csv("stopword.csv")
```


```{r eval=F}
# 불용어 제거하기 - filter()
count_word <- count_word %>%
  filter(!word %in% stopword$word)

# 불용어 제거하기 - dplyr::anti_join()
count_word <- count_word %>%
  anti_join(stopword, by = "word")
```

]


---


#### LDA 모델 만들기

##### 1. Document-Term Matrix 만들기
- **DTM(Document-Term Matrix, 문서 단어 행렬)**: 행은 문서, 열은 단어로 구성해 빈도를 나타낸 행렬

##### 1.1 문서별 단어 빈도 구하기

```{r}
# 문서별 단어 빈도 구하기
count_word_doc <- count_word %>%
  count(id, word, sort = T)
```


```{r eval=F}
count_word_doc
```



---


##### 1.2 DTM 만들기

- `tidytext::cast_dtm()`
  - `document` : 문서 구분 기준
  - `term` : 단어
  - `value` : 단어 빈도

```{r eval=F}
install.packages("tm")

# DTM 만들기
dtm_comment <- count_word_doc %>%
  cast_dtm(document = id, term = word, value = n)

dtm_comment
```
`r fontawesome("lightbulb")`  `tm` 패키지가 설치 필요



---

#### 2. LDA 모델 만들기 - `LDA()`


- `topicmodels::LDA()`
  - `k` : 토픽 수
  - `method` : 샘플링 방법. 일반적으로 깁스 샘플링(`"Gibbs"`) 가장 많이 사용
  - `control = list(seed = 1234))` : 난수 고정

```{r eval=F}
install.packages("topicmodels")
library(topicmodels)

# 토픽 모델 만들기
lda_model <- LDA(dtm_comment,
                 k = 8,
                 method = "Gibbs",
                 control = list(seed = 1234))
lda_model
```

`r fontawesome("lightbulb")`  토픽 수에는 정해진 정답이 없기 때문에 `k`값을 바꿔가며 여러 모델을 만든 다음 결과를 비교해 결정

```{r echo=F, output.lines = 12}
# install.packages("topicmodels")
library(topicmodels)

# 토픽 모델 만들기
lda_model <- LDA(dtm_comment,
                 k = 8,
                 method = "Gibbs",
                 control = list(seed = 1234))
lda_model
```

---

```{r eval=F}
# 모델 내용 확인
glimpse(lda_model)
```

```{r echo=F, output.lines = 12}
# 모델 내용 확인
glimpse(lda_model, width = 50)
```


> [편집] 출력 결과 행 박스 처리로 강조 ..@ beta  : num [1:8, 1:5995] ..@ gamma : num [1:3203, 1:8], 결과 생략 표시

- beta(β)
  - 단어가 각 토픽에 등장할 확률
  - `@ beta : num [1:8, 1:5995]` : 5,995개 단어로 모델
- amma(γ)
  - 문서가 각 토픽에 등장할 확률
  - `@ gamma : num [1:3203, 1:8]` : 3,203개 문서로 모델 생성


`r fontawesome("lightbulb")` 깁스 샘플링에 관해 자세히 알고 싶다면 - Topic Modeling, LDA: [bit.ly/easytext_61](https://bit.ly/easytext_61)

  [ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/](ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/)


