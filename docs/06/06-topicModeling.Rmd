---
title: "Do it! 쉽게 배우는 R 텍스트 마이닝 - 06 토픽 모델링: 어떤 주제로 글을 썼을까?"

author: "김영우"
output:
  xaringan::moon_reader:
    seal: false
    css: ["default", "../css/custom.css"]
    lib_dir: libs
    chakra: ../libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      ratio: '16:10'
      navigation:
        scroll: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, 
        width = 80,
        # width = 70,
        
        max.print = 80,
        tibble.print_max = 40,
        
        tibble.width = 80,
        # tibble.width = 70,
        
        # pillar.min_chars = Inf, # tibble 문자 출력 제한
        servr.interval = 0.01) # Viewer 수정 반영 속도


knitr::opts_chunk$set(cache = T, warning = F, message = F, 
                      dpi = 300, fig.height = 4)
                      # out.width = "100%"

# xaringanExtra::use_tile_view()

library(knitr)
library(icon)
library(here)
```


```{r echo=FALSE}
rm(list = ls())

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()
showtext_opts(dpi = 96)
# showtext_opts(dpi = 300) # opts_chunk$set(dpi=300)

# code highlighting
hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})

# 출력 결과 제한 함수
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n")
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```


class: title0

Do it! 쉽게 배우는 R 텍스트 마이닝

---

class: no-page-num

<br>

.pull-left[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
```{r, echo=FALSE, out.width="70%", out.height="70%"}
include_graphics("https://raw.githubusercontent.com/youngwoos/Doit_textmining/main/cover.png")
```
]

.pull-right[

<br>
<br>
<br>

`r fontawesome("github")` [github.com/youngwoos/Doit_textmining](https://github.com/youngwoos/Doit_textmining)

`r fontawesome("facebook-square")` [facebook.com/groups/datacommunity](https://facebook.com/groups/datacommunity)

- [네이버책](https://book.naver.com/bookdb/book_detail.nhn?bid=17891971)
  - [yes24](http://bit.ly/3oUuJOB)
  - [알라딘](http://bit.ly/3oXOSDn)
  - [교보문고](https://bit.ly/2LtNOcB)
]

---

class: title0

06 토픽 모델링:  
어떤 주제로 글을 썼을까?

---

class: title0-2

<br-back-20>

We'll make

<br-back-40>

```{r, echo=FALSE, out.width="60%", out.height="60%"}
include_graphics("../Image/06/06_4_1.png")
```

---

class: title0-2

<br-back-40>

and

<br-back-40>

```{r, echo=F, out.width="65%", out.height="65%"}
include_graphics("../Image/06/06_5_1_edit.png")
```

---

<br>

.large2[.font-jua[목차]]

.large[.font-jua[06-1 토픽 모델링 개념 알아보기]]([link](#06-1))

.large[.font-jua[06-2 LDA 모델 만들기]]([link](#06-2))

.large[.font-jua[06-3 토픽별 주요 단어 살펴보기]]([link](#06-3))

.large[.font-jua[06-4 문서를 토픽별로 분류하기]]([link](#06-4))

.large[.font-jua[06-5 토픽 이름 짓기]]([link](#06-5))

.large[.font-jua[06-6 최적의 토픽 수 정하기]]([link](#06-6))


---

name: 06-1
class: title1

06-1 토픽 모델링 개념 알아보기

---

#### 토픽 모델링(topic modeling)
  - 텍스트의 핵심 주제를 찾아 비슷한 내용끼리 분류하는 분석 방법
  - 분석할 텍스트가 많을 때 유용
  
```{r echo=FALSE, out.width = '70%'}
knitr::include_graphics("../Image/06/06_3_1.png")
```


---

##### 토픽 모델 예시: 문서 3개로 만든 모델

.pull-left-60[

<br10>

- 문서의 토픽
  - 문서 1: 고양이 관련 내용
  - 문서 2: 음식 관련 내용
  - 문서 3: 고양이, 음식 모두 관련 내용
]


<br10>

.pull-right-40[

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_1.png")
```
]


--


.pull-left-60[
<br>
<br>
<br>

- 토픽 모델을 이용하면
  - 단어가 어떤 토픽에 등장할 확률이 더 높은지 알 수 있다
  - 단어 등장 확률을 보고 토픽의 핵심 단어를 알 수 있다
]

.pull-right-40[
<br>

```{r echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_2.png")
```
]


---

- 토픽 모델을 이용하면
  - 문서가 어떤 토픽에 등장할 확률이 높은지 알 수 있다
  - 확률을 이용해 문서를 토픽별로 분류할 수 있다 → 다량의 문서 분석할 때 특히 유용
  - 문서가 어떤 주제로 구성되는지 파악할 수 있다

<br10>

```{r echo=FALSE, out.width = '25%', fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_3.png")
```


---


#### LDA 모델
- LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당): 가장 널리 사용되는 토픽 모델링 알고리즘


##### LDA 모델의 가정 1. 토픽은 여러 단어의 혼합으로 구성된다

- 한 토픽에 여러 단어가 서로 다른 확률로 포함된다
- 같은 단어가 여러 토픽에 서로 다른 확률로 포함된다

<br10>

```{r echo=FALSE, out.width = '25%', fig.align='center'}
knitr::include_graphics("../Image/etc/06_1_table_4.png")
```

---

##### LDA 모델의 가정 2. 문서는 토픽들의 혼합으로 구성된다

- 문서에는 여러 토픽의 단어가 서로 다른 비율로 들어 있음
- 단어 확률이 더 높은 쪽으로 문서 분류

<br10>

```{r echo=FALSE, out.width='50%', fig.cap="Latent Dirichlet allocation(LDA): bit.ly/easytext_62", fig.align='center'}
knitr::include_graphics("../Image/06/Blei_ICML_2012_edit.png")
```

<br>

`r fontawesome("lightbulb")` LDA 모델이 만들어지는 과정을 자세히 알고 싶다면 - Topic Modeling, LDA: [bit.ly/easytext_61](https://bit.ly/easytext_61)


---

name: 06-2
class: title1

6.2 LDA 모델 만들기


---

#### 전처리하기

##### 1. 기본적인 전처리

- **중복 문서 제거하기**: `dplyr::distinct()`
- 중복 문서가 있으면
  - 계산량 늘어나 모델 만드는 시간 오래 걸림
  - 한 토픽에 내용이 똑같은 문서가 어려 개 들어 있는 문제 생김

- **짧은 문서 제거하기**:
  - 토픽 모델은 여러 문서에 공통으로 사용된 단어를 이용해 만듦
  - 짧은 문서는 다른 문서와 공통으로 사용된 단어가 적어 모델 만드는 데 적합하지 않음

---

```{r eval = F}
# 기생충 기사 댓글 불러오기
library(readr)
library(dplyr)

raw_news_comment <- read_csv("news_comment_parasite.csv") %>%
  mutate(id = row_number())

library(stringr)
library(textclean)

# 기본적인 전처리
news_comment <- raw_news_comment %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply)) %>%

  # 중복 댓글 제거
  distinct(reply, .keep_all = T) %>%

  # 짧은 문서 제거 - 3 단어 이상 추출
  filter(str_count(reply, boundary("word") >= 3))
```

```{r echo = F}
# 기생충 기사 댓글 불러오기
library(readr)
library(dplyr)

raw_news_comment <- read_csv("../../Data/news_comment_parasite.csv") %>%
  mutate(id = row_number())

library(stringr)
library(textclean)

# 기본적인 전처리
news_comment <- raw_news_comment %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply)) %>%

  # 중복 댓글 제거
  distinct(reply, .keep_all = T) %>%

  # 짧은 문서 제거 - 3 단어 이상 추출
  filter(str_count(reply, boundary("word") >= 3))
```

`r fontawesome("lightbulb")` `row_number()`: 문서를 토픽별로 분류하는 작업을 할 때 문서 구분 기준이 필요하므로 댓글 고유 번호 부여

---

##### 2. 명사 추출하기

- 명사 추출: 문서의 주제는 명사로 결정되므로 명사 추출해 모델 만드는 경우가 많음
- 각 댓글에서 중복 사용된 단어 제거: 문서에 같은 단어가 여러 번 사용되면 내용과 관계없이 단순히 사용 빈도 때문에 특정 토픽으로 분류될 가능성이 높아짐

```{r}
library(tidytext)
library(KoNLP)

# 명사 추출
comment <- news_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = extractNoun,
                drop = F) %>%
  filter(str_count(word) > 1) %>%

  # 댓글 내 중복 단어 제거
  group_by(id) %>%
  distinct(word, .keep_all = T) %>%
  ungroup() %>%
  select(id, word)
```


```{r eval=F}
comment
```

---

##### 3. 빈도가 높은 단어 제외하기

- '영화', '기생충' 등은 거의 모든 댓글에 들어 있음
- 빈도가 매우 높은 단어가 포함된 상태로 토픽 모델을 만들면 대부분의 토픽에 똑같은 단어가 주요 단어로 등장해 토픽의 특징을 파악하기 어려우므로 제거

```{r}
count_word <- comment %>%
  add_count(word) %>%
  filter(n <= 200) %>%
  select(-n)
```

---

##### 4. 불용어 제거하기, 유의어 처리하기

**4.1 불용어, 유의어 확인하기**
- **불용어(Stop word)**: 분석에서 제외할 단어
  - `"들이"`, `"하다"`, `"하게"`처럼 의미를 알 수 없는 단어
  - 텍스트 해석에 도움이 되지 않으므로 제거해야 함
- 빈도가 높은 단어 추출해 불용어 확인, 표현은 다르지만 의미가 비슷한 유의어가 있는지 확인

```{r eval=F}
# 불용어, 유의어 확인하기
count_word %>%
  count(word, sort = T) %>%
  print(n = 200)
```

```{r echo=F}
# 불용어, 유의어 확인하기
count_word %>%
  count(word, sort = T) %>%
  print(n = 5)
```

---

**4.2 불용어 목록 만들기**

```{r}
# 불용어 목록 만들기
stopword <- c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
              "해요", "이것", "니들", "하기", "하지", "한거", "해주",
              "그것", "어디", "여기", "까지", "이거", "하신", "만큼")
```

--


**4.3 불용어 제거하고 유의어 수정하기**

- `dplyr::recode()`: 특정 값을 다른 값으로 수정하는 함수

```{r}
# 불용어, 유의어 처리하기
count_word <- count_word %>%
  filter(!word %in% stopword) %>%
  mutate(word = recode(word,
                       "자랑스럽습니" = "자랑",
                       "자랑스럽" = "자랑",
                       "자한" = "자유한국당",
                       "문재" = "문재인",
                       "한국의" = "한국",
                       "그네" = "박근혜",
                       "추카" = "축하",
                       "정경" = "정경심",
                       "방탄" = "방탄소년단"))
```

---


> [알아두면 좋아요] 불용어 목록을 파일로 만들어 활용하면 편리합니다.

불용어를 제외하는 작업은 텍스트를 분석할 때마다 반복하기 때문에 자주 등장하는 불용어를 CSV 파일로 저장해두고 필요할 때마다 불러와서 활용하면 편리합니다.

```{r eval=F}
# tibble 구조로 불용어 목록 만들기
stopword <- tibble(word = c("들이", "하다", "하게", "하면", "해서", "이번", "하네",
                            "해요", "이것", "니들", "하기", "하지", "한거", "해주",
                            "그것", "어디", "여기", "까지", "이거", "하신", "만큼")

# 불용어 목록 저장하기
library(readr)
write_csv(stopword, "stopword.csv")

# 불용어 목록 불러오기
stopword <- read_csv("stopword.csv")

# 불용어 제거하기
count_word <- count_word %>%
  filter(!word %in% stopword$word)
```

`stopword`는 tibble 구조이기 때문에 다음과 같이 `anti_join()`을 이용해서 불용어를 제거할 수 있습니다.

```{r eval=F}
count_word <- count_word %>%
  anti_join(stopword, by = "word")
```

---


### 6.2.2 LDA 모델 만들기

#### 1. Document-Term Matrix 만들기

LDA 모델은 **DTM(Document-Term Matrix, 문서 단어 행렬)**을 이용해 만듭니다. DTM은 행은 각 문서, 열은 각 단어로 구성해 빈도를 나타낸 행렬 자료입니다.

#### (1) 문서별 단어 빈도 구하기

DTM은 문서별 단어 빈도를 이용해 만듭니다. `count_word`를 이용해 문서별 단어 빈도를 구하겠습니다.

```{r}
# 문서별 단어 빈도 구하기
count_word_doc <- count_word %>%
  count(id, word, sort = T)
```


```{r eval=F}
count_word_doc
```


```{r echo=F}
count_word_doc %>%
  print(n = 5)
```


#### (2) DTM 만들기 - `cast_dtm()`

`tidytext` 패키지의 `cast_dtm()`에 문서별 단어 빈도를 적용하면 DTM을 만들 수 있습니다. `cast_dtm()`에는 아래 세 가지 파라미터를 입력합니다.

- `document` : 문서를 구분하는 기준이 되는 변수
- `term` : 단어가 들어있는 변수
- `value` : 단어 빈도가 들어 있는 변수


`cast_dtm()`을 이용하려면 `tm`패키지가 설치되어 있어야 합니다. `tm`패키지를 설치한 다음 `cast_dtm()`을 이용해 `count_word_doc`을 이용해 DTM을 만들겠습니다. 출력 결과의 `(documents: 3203, terms: 5995)`를 보면, `dtm_comment`가 '3,203 문서 × 5,995 단어'로 구성됨을 알 수 있습니다.

```{r eval=F}
install.packages("tm")
```

```{r}
# DTM 만들기
dtm_comment <- count_word_doc %>%
  cast_dtm(document = id, term = word, value = n)

dtm_comment
```

> [편집] 박스로 강조 표시 (documents: 3203, terms: 5995)

<br>

> [참고] `as.matrix()`를 이용하면 DTM의 내용을 확인할 수 있습니다. 출력 결과에서 `Docs`는 문서 번호, `Terms`는 단어, 숫자는 문서에 단어가 등장한 빈도를 의미합니다.

```{r}
as.matrix(dtm_comment)[1:8, 1:8]
```





#### 2. LDA 모델 만들기 - `LDA()`

`topicmodels` 패키지의 `LDA()`에 DTM을 적용하면 LDA 모델을 만들 수 있습니다. `LDA()`에는 아래 세 가지 파라미터를 입력합니다.

- `k` : 토픽 수. 여기서는 8개의 토픽으로 모델을 만들도록 `8`을 입력하겠습니다.

- `method` : 샘플링 방법. 토픽 모델링은 샘플링을 반복하며 토픽과 단어의 분포를 추정하는 과정을 거칩니다. 여기서는 가장 일반적으로 사용되는 깁스 샘플링을 이용하도록 `"Gibbs"`를 입력하겠습니다.

- `control = list(seed = 1234))` : 반복 실행해도 동일한 결과를 만들도록 난수를 고정합니다.

다음 코드를 실행하면 LDA 모델이 만들어 집니다. `glimpse()`를 이용하면 LDA 모델의 내용을 확인할 수 있습니다.

> [참고] 토픽 수에는 정해진 정답이 없기 때문에 `k`값을 바꿔가며 여러 모델을 만든 다음 결과를 비교해 결정하게 됩니다. 토픽 수를 정하는 방법은 `6.6`에서 자세히 다룹니다.

```{r eval=F}
install.packages("topicmodels")
library(topicmodels)

# 토픽 모델 만들기
lda_model <- LDA(dtm_comment,
                 k = 8,
                 method = "Gibbs",
                 control = list(seed = 1234))
lda_model
```


```{r echo=F, output.lines = 12}
# install.packages("topicmodels")
library(topicmodels)

# 토픽 모델 만들기
lda_model <- LDA(dtm_comment,
                 k = 8,
                 method = "Gibbs",
                 control = list(seed = 1234))
lda_model
```

```{r eval=F}
# 모델 내용 확인
glimpse(lda_model)
```

```{r echo=F, output.lines = 12}
# 모델 내용 확인
glimpse(lda_model, width = 50)
```

> [편집] 출력 결과 행 박스 처리로 강조 ..@ beta  : num [1:8, 1:5995] ..@ gamma : num [1:3203, 1:8], 결과 생략 표시

`lda_model`은 '단어가 각 토픽에 등장할 확률 beta(β)'와 '문서가 각 토픽에 등장할 확률 gamma(γ)'를 지니고 있습니다.

5,995개 단어로 토픽 모델을 만들었기 때문에 출력 결과의 `@ beta : num [1:8, 1:5995]`를 보면 8개 토픽 각각에  5,995개 베타값이 있음을 알 수 있습니다. 또한 3,203개 문서로 토픽 모델을 만들었기 때문에 `@ gamma : num [1:3203, 1:8]`를 보면 8개 토픽 각각에 3,203개 감마값이 있음을 알 수 있습니다.



> [참고] 깁스 샘플링에 관해 자세히 알고 싶다면 아래 글을 읽어보세요.

- Topic Modeling, LDA

  [ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/](ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/)
