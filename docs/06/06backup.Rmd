

<!-- ## 6.3 토픽별 주요 단어 살펴보기 -->

<!-- **베타(beta, β)**는 '단어가 각 토픽에 등장할 확률'을 의미합니다. 베타를 이용하면 각 토픽에 등장할 가능성이 높은 주요 단어를 알 수 있습니다. -->


<!-- ### 6.3.1 토픽별 단어 확률 beta 추출하기 - `tidy()` -->

<!-- `tidytext` 패키지의 `tidy()`를 이용하면 `lda_model`에서 베타와 감마를 추출할 수 있습니다. -->

<!-- #### beta 추출하기 -->

<!-- `tidy()`에 `matrix = "beta"`를 입력해 베타를 추출하겠습니다. 출력 결과의 `beta`는 단어가 각 토픽에 등장할 확률을 의미합니다. 첫 번째 행을 보면 `"한국"`이 토픽 `1`에 등장할 확률이 0.000405, 토픽 `2`에 등장할 확률이 0.0000364임을 알 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- term_topic <- tidy(lda_model, matrix = "beta") -->
<!-- term_topic -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- term_topic <- tidy(lda_model, matrix = "beta") -->
<!-- term_topic %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- #### beta 살펴보기 -->

<!-- 토픽 모델을 5,995개 단어로 만들었으므로 `topic`의 빈도를 구하면 토픽별로 5,995 행이 있음을 알 수 있습니다. `beta`는 확률값이기 때문에 한 토픽의 `beta`를 모두 더하면 `1`이 됩니다. -->

<!-- ```{r} -->
<!-- # 토픽별 단어 수 -->
<!-- term_topic %>% -->
<!--   count(topic) -->

<!-- # 토픽 1의 beta 합계 -->
<!-- term_topic %>% -->
<!--   filter(topic == 1) %>% -->
<!--   summarise(sum_beta = sum(beta)) -->

<!-- term_topic %>% -->
<!--   filter(topic == 1) %>% -->
<!--   summarise(sum_beta = sum(beta)) -->
<!-- ``` -->

<!-- #### 특정 단어의 토픽별 확률 살펴보기 -->

<!-- 특정 단어를 추출하면 이 단어가 어떤 토픽에 등장할 확률이 높은지 알 수 있습니다. 다음 코드의 출력 결과를 보면 `"작품상"`은 토픽 6에 등장할 확률이 `0.0695`로 가장 높습니다. -->

<!-- ```{r} -->
<!-- term_topic %>% -->
<!--   filter(term == "작품상") -->
<!-- ``` -->


<!-- ### 6.3.2 토픽별 주요 단어 살펴보기 -->

<!-- 토픽에 등장할 확률이 높은 단어를 살펴보면 토픽의 특징을 이해할 수 있습니다. -->

<!-- #### 특정 토픽에서 `beta`가 높은 단어 살펴보기 -->

<!-- 6번 토픽을 추출해 `beta` 기준으로 내림차순 정렬해 출력하겠습니다. `"작품상"`, `"감독상"`, `"수상"` 등의 확률이 높은 것으로 보아 토픽 6이 영화상 수상과 관련됨을 알 수 있습니다. -->

<!-- ```{r} -->
<!-- term_topic %>% -->
<!--   filter(topic == 6) %>% -->
<!--   arrange(-beta) -->
<!-- ``` -->

<!-- #### 모든 토픽의 주요 단어 살펴보기 - `terms()` -->

<!-- `topicmodels` 패키지의 `terms()`를 이용하면 토픽별로 등장 확률이 높은 단어를 한 눈에 확인할 수 있습니다. -->


<!-- ```{r eval=F} -->
<!-- terms(lda_model, 20) %>% -->
<!--   data.frame() -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- terms(lda_model, 20) %>% -->
<!--   data.frame() %>% -->
<!--   head(5) %>% -->
<!--   select(1:4) -->
<!-- ``` -->

<!-- > [편집] 결과 생략 표시 -->

<!-- ```{r echo=F} -->
<!-- terms(lda_model, 20) %>% -->
<!--   data.frame() %>% -->
<!--   head(5) %>% -->
<!--   select(5:8) -->
<!-- ``` -->
<!-- > [편집] 결과 생략 표시 -->


<!-- ### 6.3.3 토픽별 주요 단어 시각화하기 -->

<!-- 토픽별 주요 단어의 확률을 이용해 막대 그래프를 만들겠습니다. -->

<!-- #### 1. 토픽별로 `beta`가 가장 높은 단어 추출하기 -->

<!-- 우선 토픽별로 `beta`가 가장 높은 단어를 10개씩 추출하겠습니다. -->

<!-- ```{r} -->
<!-- # 토픽별 beta 상위 10개 단어 추출 -->
<!-- top_term_topic <- term_topic %>% -->
<!--   group_by(topic) %>% -->
<!--   slice_max(beta, n = 10) -->

<!-- ``` -->

<!-- > [참고] 토픽별 단어 확률에 동점이 있으면 추출한 단어가 10개를 넘길 수 있습니다. 동점을 제외하고 토픽별 단어 수를 동일하게 맞추려면 `slice_max()`에 `with_ties = F`를 입력하면 됩니다. -->

<!-- #### 2. 막대 그래프 만들기 -->

<!-- 추출한 `top_term_topic`을 이용해 막대 그래프를 만들겠습니다. 토픽이 8개이므로 4열, 2행으로 출력하도록 `facet_wrap`의 `ncol`에 `4`를 입력하겠습니다. -->
<!-- ```{r eval=F} -->
<!-- install.packages("scales") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library(scales) -->
<!-- library(ggplot2) -->

<!-- ggplot(top_term_topic, -->
<!--        aes(x = reorder_within(term, beta, topic), -->
<!--            y = beta, -->
<!--            fill = factor(topic))) + -->
<!--   geom_col(show.legend = F) + -->
<!--   facet_wrap(~ topic, scales = "free", ncol = 4) + -->
<!--   coord_flip() + -->
<!--   scale_x_reordered() + -->
<!--   scale_y_continuous(n.breaks = 4, -->
<!--                      labels = number_format(accuracy = .01)) +  -->
<!--   labs(x = NULL) + -->
<!--   theme(text = element_text(family = "nanumgothic")) -->
<!-- ``` -->


<!-- > [꿀팁] `scale_y_continuous()`에 입력한 `n.breaks = 4`는 축의 눈금을 4개 내외로 정하는 기능을 합니다. `labels = number_format(accuracy = .01)`은 눈금 숫자를 소수점 첫째 자리에서 반올림하는 기능을 합니다. `number_format()`을 사용하려면 `scales` 패키지를 로드해야 합니다. -->

<!-- --- -->


<!-- > [편집] 그래프 가로 폭 넓게 -->

<!-- 등장 확률이 높은 주요 단어를 보면 토픽이 어떤 특징을 지니는지 알 수 있습니다. 예를 들어 토픽 4의 `"역사"`, `"우리나라"`, `"세계"`를 보면 이 토픽이 아카데미상 수상의 역사적 의미와 관련됨을 알 수 있습니다. 토픽 7의 `"블랙리스트"`, `"박근혜"`, `"자유한국당"`을 보면 이 토픽이 정치 문제와 관련됨을 알 수 있습니다. -->


<!-- ## 6.4 문서를 토픽별로 분류하기 -->

<!-- 주요 단어를 보면 각 토픽이 어떤 특징을 지니는지 가늠할 수 있지만 구체적으로 이해하기는 어렵습니다. 토픽을 자세히 이해하려면 주요 단어와 원문을 함께 살펴봐야 합니다. -->

<!-- 토픽의 원문을 살펴보려면 우선 문서를 토픽별로 분류해야 합니다. '문서가 각 토픽에 등장할 확률'인 **감마(gamma, γ)**를 이용하면 문서를 토픽별로 분류할 수 있습니다. -->



<!-- ### 6.4.1 문서별 토픽 확률 gamma 추출하기 - `tidy()` -->

<!-- `tidytext` 패키지의 `tidy()`를 이용하면 `lda_model`에서 감마를 추출할 수 있습니다. -->

<!-- #### gamma 추출하기 -->

<!-- `tidy()`를 이용해 `lda_model`에서 베타를 추출했던 것처럼, 이번에는 `matrix = "gamma"`를 입력해 감마를 추출하겠습니다. -->

<!-- ```{r eval=F} -->
<!-- doc_topic <- tidy(lda_model, matrix = "gamma") -->
<!-- doc_topic -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- doc_topic <- tidy(lda_model, matrix = "gamma") -->
<!-- doc_topic %>% print(n = 5) -->
<!-- ``` -->

<!-- #### gamma 살펴보기 -->

<!-- 토픽 모델을 3,203개 문서로 만들었으므로 `topic`의 빈도를 구하면 토픽별로 3,203 행이 있음을 알 수 있습니다. `gamma`도 `beta`와 마찬가지로 확률값이기 때문에 한 문서의 `gamma`를 모두 더하면 `1`이 됩니다. -->


<!-- ```{r} -->
<!-- doc_topic %>% -->
<!--   count(topic) -->

<!-- # 문서 1의 gamma 합계 -->
<!-- doc_topic %>% -->
<!--   filter(document == 1) %>% -->
<!--   summarise(sum_gamma = sum(gamma)) -->
<!-- ``` -->

<!-- ### 6.4.2 문서를 확률이 가장 높은 토픽으로 분류하기 -->

<!-- 문서별로 `gamma`가 가장 높은 토픽을 추출하면 문서가 어떤 토픽에 등장할 확률이 높은지 알 수 있습니다. 이렇게 얻은 값을 이용하면 각 문서를 확률이 높은 토픽으로 분류할 수 있습니다. -->


<!-- #### 1. 문서별로 확률이 가장 높은 토픽 추출하기 -->

<!-- 먼저 문서별로 `gamma`가 가장 높은 토픽을 추출하겠습니다. -->

<!-- ```{r eval=F} -->
<!-- # 문서별로 확률이 가장 높은 토픽 추출 -->
<!-- doc_class <- doc_topic %>% -->
<!--   group_by(document) %>% -->
<!--   slice_max(gamma, n = 1) -->

<!-- doc_class -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 문서별로 확률이 가장 높은 토픽 추출 -->
<!-- doc_class <- doc_topic %>% -->
<!--   group_by(document) %>% -->
<!--   slice_max(gamma, n = 1) -->

<!-- doc_class %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- #### 2. 원문에 토픽 번호 부여하기 -->

<!-- 원문이 들어있는 `raw_news_comment`에 `doc_class`를 결합하면 원문에 토픽 번호를 부여할 수 있습니다. `raw_news_comment`의 `id`와 `doc_class`의 `document`가 문서 번호를 나타낸 식별 값이므로 이 변수를 기준으로 결합하겠습니다. -->

<!-- 우선 `doc_class`의 `document`가 `chr` 타입이므로 `raw_news_comment`의 `id`와 동일하게 `integer` 타입으로 변환하겠습니다. 그런 다음 `left_join()`을 이용해 결합하겠습니다. -->

<!-- ```{r} -->
<!-- # integer로 변환 -->
<!-- doc_class$document <- as.integer(doc_class$document) -->

<!-- # 원문에 토픽 번호 부여 -->
<!-- news_comment_topic <- raw_news_comment %>% -->
<!--   left_join(doc_class, by = c("id" = "document")) -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- # 결합 확인 -->
<!-- news_comment_topic %>% -->
<!--   select(id, topic) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 결합 확인 -->
<!-- news_comment_topic %>% -->
<!--   select(id, topic) %>% print(n = 5) -->
<!-- ``` -->


<!-- #### 3. 토픽별 문서 수 살펴보기 -->

<!-- `news_comment_topic`을 이용해 토픽별 빈도를 구하면 각 토픽에 문서가 몇 개씩 있는지 알 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- news_comment_topic %>% -->
<!--   count(topic) -->
<!-- ``` -->


<!-- ```{r echo=F} -->
<!-- news_comment_topic %>% -->
<!--   count(topic) %>%  -->
<!--   print(n = Inf) -->
<!-- ``` -->

<!-- `topic`이 `NA`인 문서가 있는 이유는 고빈도 단어를 제외하는 전처리 작업을 거치지 않은 `raw_news_comment`에 토픽 번호를 부여했기 때문입니다. 토픽 모델에 사용되지 않은 문서까지 포함하고 있으므로 `left_join()`을 이용해 토픽 번호를 부여하는 과정에서 `NA`가 만들어진 것입니다. `topic`이 `NA`인 문서는 분석에 활용하지 않을 것이므로 `na.omit()`을 이용해 제거하겠습니다. -->

<!-- ```{r} -->
<!-- news_comment_topic <- news_comment_topic %>% -->
<!--   na.omit() -->
<!-- ``` -->

<!-- --- -->

<!-- > [알아두면 좋아요] 문서당 토픽 1개씩만 남기기 -->

<!-- 어떤 문서는 `gamma`가 가장 높은 토픽이 여러 개일 수 있습니다. 다음 코드를 실행하면 `doc_topic`에서 `gamma`가 가장 높은 토픽을 하나씩 추출했는데도 둘 이상인 행이 다수 존재한다는 것을 알 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- doc_topic %>% -->
<!--   group_by(document) %>% -->
<!--   slice_max(gamma, n = 1) %>% -->
<!--   count(document) %>% -->
<!--   filter(n >= 2) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- doc_topic %>% -->
<!--   group_by(document) %>% -->
<!--   slice_max(gamma, n = 1) %>% -->
<!--   count(document) %>% -->
<!--   filter(n >= 2) %>% -->
<!--   print(n = 5) -->
<!-- ``` -->

<!-- `slice_sample()`을 이용해 문서당 한 행씩 무작위 추출하면 문서당 토픽을 하나씩만 남길 수 있습니다. 이렇게 하면 토픽이 하나인 문서는 그대로 남고, 토픽이 둘 이상인 문서는 무작위로 한 행씩만 남습니다. 즉, `gamma`가 가장 높은 토픽이 여러 개인 경우 문서를 한 토픽에 무작위 배정하는 것입니다. `doc_class_unique`를 출력하면 토픽 모델을 만드는데 사용된 문서 수와 동일하게 3,203 행임을 알 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- set.seed(1234) -->
<!-- doc_class_unique <- doc_topic %>% -->
<!--   group_by(document) %>% -->
<!--   slice_max(gamma, n = 1) %>% -->
<!--   slice_sample(n = 1) -->

<!-- doc_class_unique -->
<!-- ``` -->


<!-- ```{r echo=F} -->
<!-- set.seed(1234) -->
<!-- doc_class_unique <- doc_topic %>% -->
<!--   group_by(document) %>% -->
<!--   slice_max(gamma, n = 1) %>% -->
<!--   slice_sample(n = 1) %>% -->
<!--   print(n = 5) -->

<!-- doc_class_unique -->
<!-- ``` -->

<!-- > [참고] `slice_sample()`은 난수를 이용하기 때문에 `set.seed()`를 먼저 실행해야 동일한 결과를 만들 수 있습니다. -->

<!-- 문서를 한 토픽에만 배정했으므로 `document`로 빈도를 구하면 모든 문서가 1행씩만 남아 있음을 알 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- # 문서 빈도 구하기 -->
<!-- doc_class_unique %>% -->
<!--   count(document, sort = T) -->
<!-- ``` -->


<!-- ```{r echo=F} -->
<!-- # 문서 빈도 구하기 -->
<!-- doc_class_unique %>% -->
<!--   count(document, sort = T) %>% -->
<!--   print(n = 5) -->
<!-- ``` -->

<!-- --- -->


<!-- ### 6.4.3 토픽별 문서 수와 단어 시각화하기 -->

<!-- 문서 수와 주요 단어를 막대 그래프로 함께 표현하면 전체 문서가 토픽별로 어떻게 구성되는지 한 눈에 파악할 수 있습니다. -->

<!-- #### 1. 토픽별 주요 단어 목록 만들기 -->

<!-- 토픽별 단어 확률 `beta`가 들어있는 `term_topic`에서 토픽별로 확률이 가장 높은 주요 단어를 6개씩 추출하겠습니다. 이때, 확률이 동점인 단어는 제외하도록 `slice_max()`에 `with_ties = F`를 입력하겠습니다. 그런 다음 `summarise()`와 `paste()`를 이용해 주요 단어를 한 행으로 만들겠습니다. -->

<!-- ```{r} -->
<!-- top_terms <- term_topic %>% -->
<!--   group_by(topic) %>% -->
<!--   slice_max(beta, n = 6, with_ties = F) %>% -->
<!--   summarise(term = paste(term, collapse = ", ")) -->

<!-- top_terms -->
<!-- ``` -->

<!-- #### 2. 토픽별 문서 빈도 구하기 -->

<!-- 원문과 토픽 번호가 들어있는 `news_comment_topic`을 이용해 토픽별 문서 빈도를 구하겠습니다. -->

<!-- ```{r} -->
<!-- count_topic <- news_comment_topic %>% -->
<!--   count(topic) -->

<!-- count_topic -->
<!-- ``` -->


<!-- #### 3. 문서 빈도에 주요 단어 결합하기 -->

<!-- 앞에서 만든 `count_topic`과 `top_terms`을 결합하겠습니다. 그런 다음 막대 그래프의 x축에 `Topic 1`의 형태로 토픽 번호를 표시하기 위해 `topic_name`을 추가하겠습니다. -->

<!-- ```{r} -->
<!-- count_topic_word <- count_topic %>% -->
<!--   left_join(top_terms, by = "topic") %>% -->
<!--   mutate(topic_name = paste("Topic", topic)) -->

<!-- count_topic_word -->
<!-- ``` -->

<!-- > [편집] 출력 줄맞춤 -->

<!-- #### 4. 토픽별 문서 수와 주요 단어로 막대 그래프 만들기 -->

<!-- 토픽별 문서 수와 주요 단어를 담고 있는 `count_topic_word`를 이용해 막대 그래프를 만들겠습니다. `geom_text()`를 이용해 막대 끝에 문서 빈도를 표시하고, 막대 안에 토픽의 주요 단어를 표시하도록 설정하겠습니다. 출력한 그래프를 보면 막대에 주요 단어와 문서 수가 함께 표현되어 토픽의 특징을 한 눈에 파악할 수 있습니다. -->

<!-- ```{r} -->
<!-- ggplot(count_topic_word, -->
<!--        aes(x = reorder(topic_name, n), -->
<!--            y = n, -->
<!--            fill = topic_name)) + -->
<!--   geom_col(show.legend = F) + -->
<!--   coord_flip() + -->

<!--   geom_text(aes(label = n) ,                # 문서 빈도 표시 -->
<!--             hjust = -0.2) +                 # 막대 밖에 표시 -->

<!--   geom_text(aes(label = term),              # 주요 단어 표시 -->
<!--             hjust = 1.03,                   # 막대 안에 표시 -->
<!--             col = "white",                  # 색깔 -->
<!--             fontface = "bold",              # 두껍게 -->
<!--             family = "nanumgothic") +       # 폰트 -->

<!--   scale_y_continuous(expand = c(0, 0),      # y축-막대 간격 줄이기 -->
<!--                      limits = c(0, 820)) +  # y축 범위 -->
<!--   labs(x = NULL) -->

<!-- ``` -->



<!-- ## 6.5 토픽 이름 짓기 -->

<!-- 토픽을 번호로 표현하기 보다는 이름을 지으면 토픽의 특징을 이애하는데 도움이 됩니다. 주요 문서의 내용을 살펴보고 토픽 이름을 지은 다음 그래프를 만들어 보겠습니다. -->

<!-- ### 6.5.1 토픽별 주요 문서 살펴보고 토픽 이름 짓기 -->

<!-- 토픽별 주요 문서를 추출해 내용을 살펴본 다음 토픽 이름을 짓겠습니다. `news_comment_topic`에서 `gamma`가 높은 행을 추출하면 토픽을 대표하는 문서의 내용을 살펴볼 수 있습니다. -->

<!-- #### 1. 원문을 읽기 편하게 전처리하기, gamma가 높은 순으로 정렬하기 -->

<!-- 댓글 원문을 읽기 편하게 html 특수 문자를 제거하겠습니다. 그런 다음 `gamma`가 높은 주요 문서가 먼저 출력되도록 `gamma` 기준으로 내림차순 정렬하겠습니다. -->

<!-- ```{r} -->
<!-- comment_topic <- news_comment_topic %>% -->
<!--   mutate(reply = str_squish(replace_html(reply))) %>% -->
<!--   arrange(-gamma) -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- comment_topic %>% -->
<!--   select(gamma, reply) -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- comment_topic %>% -->
<!--   select(gamma, reply) %>% -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- #### 2. 주요 단어가 사용된 문서 살펴보기 -->

<!-- 각 토픽의 주요 단어가 사용된 댓글을 추출해 살펴보면 토픽이 어떤 내용을 담고 있는지 알 수 있습니다. 다음 코드의 출력 결과를 보면 토픽 1은 주로 작품상 수상을 축하하거나 정치적인 댓글을 비판하는 내용으로 구성된다는 것을 알 수 있습니다. -->

<!-- > [참고] `comment_topic`은 `tibble` 자료형이기 때문에 콘솔창 크기에 맞추어 일부만 출력됩니다. `pull()`을 이용하면 변수를 `vector` 타입으로 추출하므로 전체 내용을 출력할 수 있습니다. -->

<!-- ```{r, eval=F} -->
<!-- # 토픽 1 내용 살펴보기 -->
<!-- comment_topic %>% -->
<!--   filter(topic == 1 & str_detect(reply, "작품")) %>% -->
<!--   head(50) %>% -->
<!--   pull(reply) -->
<!-- ``` -->

<!-- ```{r, echo=F} -->
<!-- # 토픽 1 내용 살펴보기 -->
<!-- comment_topic %>% -->
<!--   filter(topic == 1 & str_detect(reply, "작품")) %>% -->
<!--   head(3) %>% -->
<!--   pull(reply) -->
<!-- ``` -->

<!-- > [편집] 출력 결과 생략 표시, 빈 줄 삭제 -->

<!-- ```{r, eval=F} -->
<!-- comment_topic %>% -->
<!--   filter(topic == 1 & str_detect(reply, "진심")) %>% -->
<!--   head(50) %>% -->
<!--   pull(reply) -->
<!-- ``` -->


<!-- ```{r, echo=F} -->
<!-- comment_topic %>% -->
<!--   filter(topic == 1 & str_detect(reply, "진심")) %>% -->
<!--   head(3) %>% -->
<!--   pull(reply) -->
<!-- ``` -->

<!-- > [편집] 출력 결과 생략 표시, 빈 줄 삭제 -->

<!-- ```{r, eval=F} -->
<!-- comment_topic %>% -->
<!--   filter(topic == 1 & str_detect(reply, "정치")) %>% -->
<!--   head(50) %>% -->
<!--   pull(reply) -->
<!-- ``` -->

<!-- ```{r, echo=F} -->
<!-- comment_topic %>% -->
<!--   filter(topic == 1 & str_detect(reply, "정치")) %>% -->
<!--   head(3) %>% -->
<!--   pull(reply) -->
<!-- ``` -->

<!-- > [편집] 출력 결과 생략 표시, 빈 줄 삭제 -->

<!-- <br> -->

<!-- > [참고] 토픽 2~8도 주요 단어가 사용된 댓글을 추출해 내용을 직접 확인해보세요. -->


<!-- #### 3. 토픽 이름 목록 만들기 -->

<!-- 앞에서 원문을 토대로 토픽 번호와 이름으로 구성된 '토픽 이름 목록'을 만들겠습니다. -->

<!-- ```{r} -->
<!-- # 토픽 이름 목록 만들기 -->
<!-- name_topic <- tibble(topic = 1:8, -->
<!--                      name = c("1. 작품상 수상 축하, 정치적 댓글 비판", -->
<!--                               "2. 수상 축하, 시상식 감상", -->
<!--                               "3. 조국 가족, 정치적 해석", -->
<!--                               "4. 새 역사 쓴 세계적인 영화", -->
<!--                               "5. 자랑스럽고 감사한 마음", -->
<!--                               "6. 놀라운 4관왕 수상", -->
<!--                               "7. 문화계 블랙리스트, 보수 정당 비판", -->
<!--                               "8. 한국의 세계적 위상")) -->
<!-- ``` -->


<!-- ### 6.5.2 토픽 이름과 주요 단어 시각화하기 -->

<!-- 토픽별 주요 단어가 들어있는 `top_term_topic`에 `name_topic`을 결합해 토픽 이름을 부여한 다음 막대 그래프를 만들겠습니다. 다음 코드로 출력한 그래프를 보면 각 막대 그래프 위에 토픽 이름이 표현된 것을 확인할 수 있습니다. -->

<!-- > [참고] `top_term_topic`은 **6.3.3**에서 만들었습니다. -->

<!-- ```{r eval=F} -->
<!-- # 토픽 이름 결합하기 -->
<!-- top_term_topic_name <- top_term_topic %>% -->
<!--   left_join(name_topic, name_topic, by = "topic") -->

<!-- top_term_topic_name -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # 토픽 이름 결합하기 -->
<!-- top_term_topic_name <- top_term_topic %>% -->
<!--   left_join(name_topic, name_topic, by = "topic") -->

<!-- top_term_topic_name %>% -->
<!--   print(n = 5) -->
<!-- ``` -->

<!-- > [편집] 출력 결과 줄 맞츰 -->

<!-- ```{r fig.height=9, fig.width=6} -->
<!-- # 막대 그래프 만들기 -->
<!-- ggplot(top_term_topic_name, -->
<!--        aes(x = reorder_within(term, beta, name), -->
<!--            y = beta, -->
<!--            fill = factor(topic))) + -->
<!--   geom_col(show.legend = F) + -->
<!--   facet_wrap(~ name, scales = "free", ncol = 2) + -->
<!--   coord_flip() + -->
<!--   scale_x_reordered() + -->

<!--   labs(title = "영화 기생충 아카데미상 수상 기사 댓글 토픽", -->
<!--        subtitle = "토픽별 주요 단어 Top 10", -->
<!--        x = NULL, y = NULL) + -->

<!--   theme_minimal() + -->
<!--   theme(text = element_text(family = "nanumgothic"), -->
<!--         title = element_text(size = 12), -->
<!--         axis.text.x = element_blank(), -->
<!--         axis.ticks.x = element_blank()) -->
<!-- ``` -->

<!-- ## 6.6 최적의 토픽 수 정하기 -->

<!-- `LDA()`는 토픽 수 `k`를 몇 개로 지정하는지에 따라 다른 구조의 모델을 만듭니다. 토픽 수가 너무 적으면 대부분의 단어가 한 토픽의 주요 단어가 되어 의미가 불분명하고, 반대로 너무 많으면 여러 토픽에 주요 단어가 중복되어 토픽의 개성이 잘 드러나지 않습니다. 텍스트를 파악하는데 도움이 되는 모델을 만들려면 적절한 토픽 수를 정해야 합니다. -->

<!-- ### 6.6.1 토픽 수를 정하는 방법 알아보기 -->

<!-- #### 방법1. 모델의 내용을 보고 해석 가능성을 고려해 토픽 수 정하기 -->

<!-- 토픽 수는 정답이 없기 때문에 분석가가 적당한 개수를 정해 모델을 만든 다음 내용을 보고 적절한지 판단해야 합니다. (1) 주요 단어가 토픽을 잘 대표하는지, (2) 문서가 비슷한 내용끼리 잘 분류됐는지, (3) 모델이 텍스트를 해석하는데 도움이 되는지를 보고 해석 가능성을 고려해 판단하는 것입니다. -->

<!-- 모델이 텍스트를 해석하는데 도움이 되는지를 보고 해석 가능성을 고려해 판단하는 것이 토픽 수를 정하는 가장 일반적인 방법입니다. 하지만 이 방법은 일일이 모델의 내용을 확인해야 하기 때문에 시간이 많이 소요됩니다. 텍스트의 내용에 대한 전문 지식이 없으면 모델이 타당한지 판단하기 어렵다는 한계도 있습니다. -->


<!-- #### 방법2. 여러 모델의 평가 지표를 비교해 토픽 수 정하기 -->

<!-- 토픽 수를 바꾸면서 여러 모델을 만든 다음 텍스트를 설명하는 정도를 나타낸 **복잡도(perplexity)** 지표를 비교하면 가장 나은 모델을 선택할 수 있습니다. 여러 모델의 성능 지표를 비교해 최적값을 찾는 작업을 **하이퍼파라미터 튜닝(Hyperparameter Tuning)** 이라고 합니다. -->

<!-- 하이퍼파라미터 튜닝은 전문적인 지식이 없어도 적당한 토픽 수를 정할 수 있다는 장점이 있습니다. 하지만 이 방법은 여러 모델을 만들어야 하므로 시간이 많이 소요된다는 단점이 있습니다. -->

<!-- #### 방법3. 두 가지 방법을 함께 사용하기 -->

<!-- 모델의 성능 지표가 가장 높다고 해서 텍스트를 이해하는데 가장 도움이 되는 모델이라고 할 수는 없습니다. 따라서 하이퍼파라미터 튜닝으로 몇 개의 후보 모델을 선정한 다음 그 중에서 해석 가능성이 높은 모델을 최종적으로 선택하는 게 좋습니다. -->

<!-- > [참고] 모델을 만들 때 사람이 직접 정하는 값을 하이퍼파라미터(Hyperparameter)라 합니다. 모델의 성능이 하이퍼파라미터에 따라 달라지기 때문에 최적값을 찾는 튜닝 작업을 하는 것입니다. -->

<!-- ### 6.6.2 하이퍼파라미터 튜닝으로 토픽 수 정하기 -->

<!-- #### 1. 토픽 수 바꿔가며 LDA 모델 여러 개 만들기 - `FindTopicsNumber()` -->

<!-- 하이퍼파라미터 튜닝으로 최적의 토픽 수를 정하는 방법을 알아보겠습니다. `ldatuning` 패키지의 `FindTopicsNumber()`를 이용하면 토픽 수를 바꿔가며 여러 모델을 만들어 성능 지표를 비교할 수 있습니다. `FindTopicsNumber()`에는 다음과 같은 파라미터를 입력합니다. -->

<!-- - `dtm` : Document Term Matrix. 여기서는 `6.2.2`에서 만든 `dtm_comment`를 입력하겠습니다. -->

<!-- - `topics` : 비교할 최소-최대 토픽 수. 여기서는 2~20까지 비교하도록 `2:20`을 입력하겠습니다. -->

<!-- - `return_models` : 모델 저장 여부. 기본값이 `FALSE`이기 때문에 성능 지표만 구하고 모델은 저장하지 않습니다. `TRUE`를 입력해 모델을 함께 저장하겠습니다. -->

<!-- - `control = list(seed = 1234))` : 반복 실행해도 동일한 결과를 만들도록 난수를 고정합니다. -->

<!-- `models`를 출력하면 토픽 수가 2~20인 총 19개 모델이 만들어졌음을 알 수 있습니다. `Griffiths2004`가 모델의 복잡도를 나타낸 성능 지표로, 모델이 텍스트의 구조를 잘 설명할수록 큰값을 지닙니다. -->

<!-- > [참고] 다음 코드는 19개의 LDA 모델을 만들기 때문에 컴퓨터 성능에 따라 실행하는 데 오래 걸릴 수 있습니다. -->

<!-- ```{r eval=F} -->
<!-- install.packages("ldatuning") -->
<!-- library(ldatuning) -->

<!-- models <- FindTopicsNumber(dtm = dtm_comment, -->
<!--                            topics = 2:20, -->
<!--                            return_models = T, -->
<!--                            control = list(seed = 1234)) -->

<!-- models %>% -->
<!--   select(topics, Griffiths2004) -->

<!-- ``` -->

<!-- ```{r, echo=F} -->
<!-- library(ldatuning) -->
<!-- models <- readRDS("files_etc/lda_models.rds")) -->

<!-- models %>% -->
<!--   select(topics, Griffiths2004) %>% -->
<!--   head(5) -->
<!-- ``` -->


<!-- > [편집] 결과 생략 표시 -->

<!-- <!-- data.frame이라 여기는 있어야함 --> -->


<!-- #### 2. 최적 토픽 수 정하기 -->

<!-- `models`를 `FindTopicsNumber_plot()`에 적용하면 성능 지표를 이용해 선 그래프를 만들어 줍니다. 그래프의 x축은 토픽 수를 의미하고, y축은 성능 지표를 0~1로 최대-최소 정규화(min-max normalization)한 값입니다. 모델의 성능이 좋을수록 y축의 값이 크며, 후보 모델 중 성능이 가장 좋으면 1, 가장 나쁘면 0이 됩니다. -->

<!-- ```{r fig.height=3, fig.width=6} -->
<!-- FindTopicsNumber_plot(models) -->
<!-- ``` -->

<!-- 그래프를 보면 y축의 값이 토픽 수가 8개가 될 때까지는 점차 큰 폭으로 증가하다가 그 이후로는 약간씩만 증가하거나 등락을 반복합니다. 성능이 비슷하면 단순한 모델을 사용하는 게 좋기 때문에 이 경우 토픽 수를 8개로 정하는 것이 적당합니다. 이처럼 '토픽 수를 늘려도 성능이 크게 향상되지 않고 등락을 반복하기 시작하는 기점'에서 토픽 수를 정하면 됩니다. 만약 해석 가능성을 포기하더라도 성능을 가장 중요하게 고려해야 한다면 y축이 1일 때로 토픽 수를 정하게 됩니다. -->

<!-- #### 3. 모델 추출하기 -->

<!-- `models`의 `LDA_model`에 `list` 구조로 된 토픽 모델이 담겨있습니다. 다음 코드를 이용하면 원하는 토픽 수의 모델을 추출해 활용할 수 있습니다. `optimal_model`은 `6.2.2`에서 `topicmodels` 패키지의 `LDA()`를 이용해 만든 `lda_model`과 동일한 내용을 담고 있습니다. -->


<!-- ```{r} -->
<!-- # 토픽 수가 8개인 모델 추출하기 -->
<!-- optimal_model <- models %>% -->
<!--   filter(topics == 8) %>% -->
<!--   pull(LDA_model) %>%              # 모델 추출 -->
<!--   .[[1]]                           # list 추출 -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- # optimal_model  -->
<!-- tidy(optimal_model, matrix = "beta") -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # optimal_model  -->
<!-- tidy(optimal_model, matrix = "beta") %>%    # beta 추출 -->
<!--   print(n = 5) -->
<!-- ``` -->


<!-- ```{r eval=F} -->
<!-- # lda_model -->
<!-- tidy(lda_model, matrix = "beta") -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- # lda_model -->
<!-- tidy(lda_model, matrix = "beta") %>%    # beta 추출 -->
<!--   print(n = 5) -->
<!-- ``` -->



<!-- --- -->

<!-- > [알아두면 좋아요] LDA 모델의 복잡도 지표 -->

<!-- `FindTopicsNumber()`를 이용하면 `Griffiths2004` 외에 다른 복잡도 지표도 구할 수 있습니다. 아래 자료를 참고하세요. -->

<!-- - Select number of topics for LDA model -->

<!--   [cran.r-project.org/web/packages/ldatuning/vignettes/topics.html](https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html) -->

<!-- `Griffiths2004`는 Griffiths와 Steyvers(2004)가 제안한 복잡도 지표로, 다양한 연구에서 토픽 모델의 성능을 측정하는데 활용되어 왔습니다. 지표를 자세히 알고 싶다면 아래 논문을 참고하세요. -->

<!-- - Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences 101, suppl 1: 5228–5235. -->

<!--   [doi.org/10.1073/pnas.0307752101](http://doi.org/10.1073/pnas.0307752101) -->

